[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the Centre for Applied Data Science (CADS) at HOGENT University of Applied Sciences and Arts. We specialize in extracting value from data through collection, organization, analysis, and accessibility. Our team tackles diverse data-related challenges, offering expertise in Data Engineering, AI, and Geospatial Data Science. We partner with various stakeholders to create innovative, data-driven solutions. For inquiries, collaborations, or advice, contact our experts or email us at cads@hogent.be. Explore our projects and initiatives on our website or engage with us on LinkedIn and GitHub.\nWe look forward to working with you!"
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html",
    "href": "posts/RAA-QuestionPro/index.html",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Bij het uitvoeren van online bevragingen moeten onderzoekers steeds een evenwicht vinden tussen praktisch gemak en het respecteren van de anonimiteit van respondenten. Vaak willen we gebruikmaken van verzendlijsten om uitnodigingen en herinneringen te versturen. Tegelijk moeten we vermijden dat antwoorden ooit gekoppeld kunnen worden aan de identiteit van deelnemers.\nQuestionPro biedt met Respondent Anonymity Assurance (RAA) een oplossing die precies dit mogelijk maakt.\nMeer informatie en de offici√´le handleiding vind je hier:\nüëâ https://www.questionpro.com/help/respondent-anonymity-assurance-raa.html\n\n\n\nRAA is een functionaliteit die ervoor zorgt dat contactgegevens en survey-antwoorden volledig gescheiden verwerkt worden. Het systeem laat toe dat onderzoekers:\n\nUitnodigingen en herinneringen via e-mail sturen.\n\nVerzendlijsten effici√´nt beheren, zonder dat antwoorden zichtbaar zijn op individueel niveau.\n\nData analyseren in een volledig anonieme dataset.\n\nDit betekent dat onderzoekers het beste van twee werelden krijgen: gebruiksgemak in de communicatie met deelnemers √©n naleving van ethische en juridische vereisten.\n\n\n\n\nVoor HOGENT-onderzoekers brengt dit verschillende voordelen:\n\nAnonimiteit gegarandeerd: deelnemers kunnen erop vertrouwen dat hun antwoorden niet herleidbaar zijn tot hun identiteit.\n\nEffici√´nte opvolging: uitnodigingen en herinneringen verlopen automatisch, zonder extra administratie.\n\nCompliance ingebouwd: de aanpak ondersteunt principes zoals privacy by design, dataminimalisatie en transparantie.\n\nEenvoudige verantwoording: dankzij een heldere standaardformulering kunnen onderzoekers eenvoudig uitleggen hoe de anonimiteit verzekerd wordt.\n\n\n\n\n\nOnderstaande tekst kan rechtstreeks gebruikt worden in onderzoeksdocumentatie, zoals een aanvraag bij een ethisch comit√©, een datamanagementplan of een GDPR-plan:\n\nGebruik van Respondent Anonymity Assurance (RAA) via QuestionPro\nVoor deze bevraging wordt gebruikgemaakt van de Respondent Anonymity Assurance (RAA)-functionaliteit van QuestionPro (zie handleiding).\nRespondenten ontvangen uitnodigingen en eventuele herinneringen per e-mail. Contactgegevens en antwoorden worden in het systeem strikt gescheiden verwerkt. Hierdoor is er geen mogelijkheid om antwoorden te koppelen aan de identiteit van respondenten.\nDeze aanpak garandeert anonimiteit en voldoet aan de vereisten inzake vertrouwelijkheid en bescherming van persoonsgegevens.\n\n\n\n\n\nDe RAA-functionaliteit in QuestionPro maakt het eenvoudig om vragenlijsten op te zetten die zowel praktisch bruikbaar zijn voor onderzoekers als veilig en vertrouwelijk voor respondenten.\nCADS raadt onderzoekers aan om bij elke survey waar anonimiteit essentieel is, gebruik te maken van RAA. Het zorgt ervoor dat je:\n\nEenvoudig deelnemers bereikt via verzendlijsten en reminders.\n\nAnonimiteit waarborgt door een strikte scheiding tussen contactgegevens en antwoorden.\n\nEen duidelijke verantwoording hebt voor ethische commissies, datamanagementplannen en GDPR-documentatie."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#wat-is-raa",
    "href": "posts/RAA-QuestionPro/index.html#wat-is-raa",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "RAA is een functionaliteit die ervoor zorgt dat contactgegevens en survey-antwoorden volledig gescheiden verwerkt worden. Het systeem laat toe dat onderzoekers:\n\nUitnodigingen en herinneringen via e-mail sturen.\n\nVerzendlijsten effici√´nt beheren, zonder dat antwoorden zichtbaar zijn op individueel niveau.\n\nData analyseren in een volledig anonieme dataset.\n\nDit betekent dat onderzoekers het beste van twee werelden krijgen: gebruiksgemak in de communicatie met deelnemers √©n naleving van ethische en juridische vereisten."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#waarom-is-dit-belangrijk-voor-onderzoekers",
    "href": "posts/RAA-QuestionPro/index.html#waarom-is-dit-belangrijk-voor-onderzoekers",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Voor HOGENT-onderzoekers brengt dit verschillende voordelen:\n\nAnonimiteit gegarandeerd: deelnemers kunnen erop vertrouwen dat hun antwoorden niet herleidbaar zijn tot hun identiteit.\n\nEffici√´nte opvolging: uitnodigingen en herinneringen verlopen automatisch, zonder extra administratie.\n\nCompliance ingebouwd: de aanpak ondersteunt principes zoals privacy by design, dataminimalisatie en transparantie.\n\nEenvoudige verantwoording: dankzij een heldere standaardformulering kunnen onderzoekers eenvoudig uitleggen hoe de anonimiteit verzekerd wordt."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#voorbeeldformulering-voor-onderzoeksdocumentatie",
    "href": "posts/RAA-QuestionPro/index.html#voorbeeldformulering-voor-onderzoeksdocumentatie",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Onderstaande tekst kan rechtstreeks gebruikt worden in onderzoeksdocumentatie, zoals een aanvraag bij een ethisch comit√©, een datamanagementplan of een GDPR-plan:\n\nGebruik van Respondent Anonymity Assurance (RAA) via QuestionPro\nVoor deze bevraging wordt gebruikgemaakt van de Respondent Anonymity Assurance (RAA)-functionaliteit van QuestionPro (zie handleiding).\nRespondenten ontvangen uitnodigingen en eventuele herinneringen per e-mail. Contactgegevens en antwoorden worden in het systeem strikt gescheiden verwerkt. Hierdoor is er geen mogelijkheid om antwoorden te koppelen aan de identiteit van respondenten.\nDeze aanpak garandeert anonimiteit en voldoet aan de vereisten inzake vertrouwelijkheid en bescherming van persoonsgegevens."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#conclusie",
    "href": "posts/RAA-QuestionPro/index.html#conclusie",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "De RAA-functionaliteit in QuestionPro maakt het eenvoudig om vragenlijsten op te zetten die zowel praktisch bruikbaar zijn voor onderzoekers als veilig en vertrouwelijk voor respondenten.\nCADS raadt onderzoekers aan om bij elke survey waar anonimiteit essentieel is, gebruik te maken van RAA. Het zorgt ervoor dat je:\n\nEenvoudig deelnemers bereikt via verzendlijsten en reminders.\n\nAnonimiteit waarborgt door een strikte scheiding tussen contactgegevens en antwoorden.\n\nEen duidelijke verantwoording hebt voor ethische commissies, datamanagementplannen en GDPR-documentatie."
  },
  {
    "objectID": "posts/qualtrics-powerbi/index.html",
    "href": "posts/qualtrics-powerbi/index.html",
    "title": "Integrating Qualtrics survey results into Power BI",
    "section": "",
    "text": "Introduction\nImagine you‚Äôre tasked with integrating Qualtrics survey results into Power BI for interactive dashboards and reports, data visualization, or combining multiple data sources. At first, this seems like a straightforward task. You dive into Power BI, expecting to find a ready-to-use connector for Qualtrics, only to realise none exists.\nYour journey takes an unexpected turn as you begin to explore alternative solutions. You discover that while some companies offer integration services, they charge exorbitant fees, up to $200 per month. This discovery propels you to look for a more cost-effective and efficient solution.\nThrough diligent research and exploration, you come across the R package {qualtRics}, a beacon of hope in your quest. This package, specifically designed for R, offers functions that allow you to fetch survey results directly from Qualtrics through an API. It‚Äôs a game-changer, offering a direct line to the data you need without the hefty price tag.\nYour next steps are clear but require careful planning and execution:\n\nSetting up an API token in Qualtrics: You‚Äôll navigate the Qualtrics interface to generate an API token. This token is essential for authenticating your requests and securing the data transfer.\nDownloading and installing R and the {qualtRics} package: R will serve as the backbone for data manipulation and analysis. The {qualtRics} package will be the tool that bridges Qualtrics and R, enabling you to pull the data into an environment where you can manipulate it freely.\nSetting Power BI to work with R: Power BI‚Äôs versatility allows it to integrate with R scripts, a feature you plan to leverage. You‚Äôll configure Power BI to recognise R, setting the stage for seamless data integration.\nWriting a query in Power BI to fetch data from surveys: The final piece of the puzzle involves crafting a query within Power BI that utilises R and the {qualtRics} package to fetch your desired survey data. This query will be your key to unlocking the insights hidden within your survey responses.\n\n\n\nSetting up an API Token in Qualtrics\nYour first step in this data integration journey is to obtain an API token from Qualtrics. This token acts as a key, granting you access to your survey data through the API. It ensures secure communication between Qualtrics and any external applications, like R in your case, that you might use for data analysis. Here‚Äôs how you can go about setting up an API token in Qualtrics:\n\nAccessing the Qualtrics interface: Begin by logging into your Qualtrics account. Navigate to the ‚ÄúAccount Settings‚Äù under ‚ÄúMy Account‚Äù at the top right corner of the page.\nFinding the API section: Within the ‚ÄúUser Settings‚Äù, look for a section called ‚ÄúAPI‚Äù. Qualtrics continuously updates its interface, so the exact naming might vary, but you‚Äôre looking for the area where API-related settings are managed.\nGenerating the API token: In the API section, there should be an option to ‚ÄúGenerate Token‚Äù. Clicking on this option will either generate a new token for you or take you to a screen where you can request a token.\nSecuring the token: Once your token is generated, it‚Äôs crucial to keep it secure. Treat it like a password, as it provides access to your Qualtrics data. Store it in a safe place, and avoid sharing it unnecessarily.\nGet your Data centre ID: You will also need to know your Data centre ID. You can find this under the ‚ÄúAccount Settings‚Äù in the Qualtrics interface when you look for the section ‚ÄúUser‚Äù.\n\nBy successfully setting up an API token in Qualtrics, you‚Äôve taken the first significant step towards integrating your survey data with Power BI. This token will be used in subsequent steps to authenticate your data requests and ensure a seamless flow of information between Qualtrics and your analysis tools.\n\n\nDownloading and installing R and the {qualtRics} package\nAfter securing your API token from Qualtrics, the next step is to set up the tools you‚Äôll use for fetching survey results: R and the {qualtRics} package. R is a powerful programming language used extensively in data analysis and statistical computing. The {qualtRics} package, specifically designed for R, facilitates the connection to Qualtrics, allowing you to fetch and work with your survey data seamlessly. Here‚Äôs how you can do this:\n\nDownloading R:\n\nVisit the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/.\nSelect the download link that corresponds to your operating system (Windows, Mac, or Linux).\nFollow the instructions to download and install R on your computer. The installation process is straightforward, typically involving a series of clicks through the setup wizard.\n\nRunning R and installing the {qualtRics} package: While R can be ran using a graphical interface, we will use command-line tools to install the {qualtRics} package. Here‚Äôs how you can do this:\n\nFor Windows users:\n\nOpen the Command Prompt by typing cmd in the Windows search bar.\nType R and press Enter to start an R session within the Command Prompt.\nIn the R session within the Command Prompt, type the following command and press Enter: install.packages(\"qualtRics\").\n\nFor Mac users:\n\nOpen the Terminal application (you can find it using Spotlight with Cmd + Space and then typing ‚ÄúTerminal‚Äù).\nType R and press Enter to start an R session within the Terminal.\nIn the R session within the Terminal, type the following command and press Enter: install.packages(\"qualtRics\").\n\n\n\nFor both Windows and Mac, after installing the {qualtRics} package, you can load it in any R session by typing library(qualtRics). This will enable you to use the package‚Äôs functions to connect to and fetch data from Qualtrics.\n\nSetting up Qualtrics API credentials in R:\n\nTo ensure R can communicate with Qualtrics using your API token, set your credentials within R. This involves using a command in the Command Prompt (Windows) or Terminal (Mac) such as in the example below (don‚Äôt mind the new lines, they are just for readability).\nReplace \"your_api_token_here\" with the actual API token you obtained from Qualtrics and \"your_data_centre_id_here\" with the appropriate name for your Qualtrics data centre ID.\n\n\n\nqualtRics::qualtrics_api_credentials(\n    api_key = \"your_api_token_here\", \n    base_url = \"your_data_centre_id_here.qualtrics.com\",\n    install = TRUE,\n    overwrite = TRUE\n    )\n\nBy completing these steps, you‚Äôve successfully set up R and the {qualtRics} package, and you‚Äôve configured R to communicate with Qualtrics using your API token. All the above should be done only once, and you‚Äôre now ready to use R to fetch survey data from Qualtrics and integrate it with Power BI.\n\n\nSetting Power BI to work with R\nTo unlock the power of R scripting Power BI, you first need to ensure that R is correctly referenced in your Power BI application. Open Power BI Desktop, navigate to File &gt; Options and settings &gt; Options to open the Options menu. Here, find the R scripting tab where you‚Äôll inform Power BI about your R installation. This setup requires specifying the path to the R executable in the R home directory field. Usually, Power BI will automatically detect the R installation, but if it doesn‚Äôt, you can manually specify the path to the R executable. Once you‚Äôve set up R in Power BI, you‚Äôre ready to start integrating your survey data with Power BI.\n\n\nWriting a query in Power BI to fetch data from surveys\nThe final step in integrating Qualtrics survey results into Power BI involves writing a query in Power BI to fetch the data. This step leverages the work you‚Äôve done so far. Here‚Äôs how you can write and execute a query to fetch your survey data:\n\nOpen Power BI and Start a New Query:\n\nLaunch Power BI Desktop and create a new report. Navigate to the Home tab, and select Get Data. Choose More to see all data connection options.\n\nSelect R Script as the Data Source:\n\nIn the Get Data window, scroll down or search for R script, then select it and click Connect. This opens a dialog box where you can input your R script.\n\nCraft Your R Script:\n\nIn the R script input box, you‚Äôll write an R script that utilizes the {qualtRics} package to fetch survey data from Qualtrics. R will use the API token you set up earlier to authenticate the request and fetch the data. You just need one additional parameter, and that‚Äôs the survey ID. This is the unique identifier for the survey you want to fetch data from. You can find the survey ID in the Qualtrics interface, typically in the URL when you‚Äôre viewing the survey. The survey ID is a long string of characters and numbers, and it‚Äôs unique to each survey. Here‚Äôs an example of an R script that fetches survey data using the {qualtRics} package:\n\n\n\nlibrary(qualtRics)\nmy_table &lt;- as.data.frame(\n    qualtRics::fetch_survey(surveyID = \"your_survey_ID\")\n)\n\nReplace \"your_survey_ID\" with the actual survey ID you want to fetch data from. This R script will fetch the survey data and store it in a data frame called my_table. You can then use this data frame to work with the survey data in Power BI.\n\nExecute the R Script:\n\nAfter writing your R script, click OK to execute the script. Power BI will run the R script and fetch the survey data from Qualtrics. The data will be loaded into Power BI, and you can start working with it in the Power BI interface!\n\n\nBy writing a query in Power BI to fetch data from your Qualtrics surveys, you‚Äôve effectively bridged the gap between these powerful platforms. This integration not only streamlines your workflow but also opens up new possibilities for analyzing and visualizing survey data to inform data-driven decisions."
  },
  {
    "objectID": "posts/vlaamse-spraakherkenning/index.html",
    "href": "posts/vlaamse-spraakherkenning/index.html",
    "title": "Spraakherkenning van Vlaamse dialecten tot algemeen Nederlands",
    "section": "",
    "text": "Inleiding\nSpraakherkenning is cruciaal voor toepassingen zoals automatische ondertiteling en spraakgestuurde systemen (bijvoorbeeld de voice assistent in je auto of op je smartphone). Ondanks de snelle ontwikkeling van deze technologie blijft spraakherkenning een uitdaging voor minder voorkomende talen zoals het Nederlands. Regionale variaties van de taal zorgen bovendien voor een bijkomende complexiteit. In de praktijk vertaalt zich dat in frustraties bij gebruikers wanneer ze in het Vlaams hun systemen proberen te bedienen.\nIn zijn bachelorproef onderzocht Jens Coetsiers de prestaties van huidige spraakherkenningssystemen op de Vlaamse spreektaal om hun bruikbaarheid te beoordelen. Slagen huidige speech-to-text-modellen erin een dialoog tussen twee sprekers in het Vlaams voldoende accuraat om te zetten in geschreven taal? Hoe presteren de modellen voor het Vlaams vergeleken met het Nederlands? Zijn er significante verschillen in de transcriptie van Vlaamse standaardtaal en Vlaamse regiolecten? Welke tool komt als Vlaamse spraakherkenningskampioen uit de bus voor de gegeven casus?\n\n\nOnderzoek\nOp basis van een literatuurstudie werd een keuze gemaakt uit de beschikbare data die kon gebruikt worden om de onderzoeksvragen te beantwoorden. De specifieke casus die aanleiding gaf tot het onderzoek ‚Äì nl. de vraag naar een spraakherkenningsmodel dat bruikbare transcripties oplevert voor Vlaamse √©√©n-op-√©√©ngesprekken ‚Äì was doorslaggevend voor de gemaakte keuzes.\nEr werden twee datasets geselecteerd. Enerzijds gebruikte Jens de telefoondialogen en face-to-face-gesprekken uit de Corpus Gesproken Nederlands. Deze bevatten ongeveer 27 uur aan Nederlandse en Vlaamse audiodata van zowel voorbereide als spontane conversaties, ook in bepaalde regionale variaties (de zogenaamde Vlaamse tussentaal). Dit komt overeen met ongeveer 406.000 woorden. Deze gesprekken worden gekenmerkt door hun spontaniteit en indirect karakter. Anderzijds werden ook voor 4 uur aan geluidsfragmenten uit de corpus Variatie in Nederlandse Taal en Sprekers gebruikt (ongeveer 38.000 woorden), meer bepaald de set met betekenisvolle en met betekenisloze zinnen. De variatie in voorspelbaarheid van deze zinnen verhoogt hun meerwaarde voor het onderzoek. Belangrijk om op te merken is dat in beide datasets enkel √©√©n-op-√©√©ngesprekken voorkomen en dat de geluidsfragmenten gekozen of aangepast werden zodat bepaalde verstoringen zoals ruis of slikken minder voorkwamen.\nOm te bepalen hoe goed de modellen de audio kunnen omzetten naar tekst, werd de herkende tekst vergeleken met gecontroleerde transcripties die als gold standard gelden. Per fragment werd de zogenoemde Word Error Rate (WER) berekend. Dit is een relatieve weergave van het aantal foutief herkende woorden, waarbij bijgehouden wordt hoeveel woorden worden toegevoegd, weggelaten of aangepast in een automatische transcriptie ten opzichte van de controletranscriptie. De geteste geluidsfragmenten lieten zich opsplitsen in verschillende categorie√´n: Algemeen Nederlands, Vlaamse standaardtaal en Vlaamse tussentaal. Die laatste categorie werd verder opgesplitst volgens regio: West-Vlaanderen, Oost-Vlaanderen, Antwerpen en Vlaams-Brabant, en Limburg. Per categorie werd vervolgens de gemiddelde WER berekend. Algemeen wordt aangenomen dat een WER van 5% tot 10% goed en van 10% tot 20% acceptabel is. Een nadeel van de gevolgde werkwijze is dat deze maat nogal streng wordt toegepast. Zo wordt bijvoorbeeld ‚Äúer op‚Äù niet als correcte weergave van het tussentalige ‚Äúd‚Äôrop‚Äù beschouwd. Ook worden modellen die getallen in cijfers weergeven benadeeld, aangezien de correcte tekst de getallen als voluit geschreven woorden bevat.\nNaast de keuze voor de testdata, werd ook een selectie gemaakt van de meest veelbelovende spraakmodellen voor de gegeven casus. Mee bepalend voor de keuze waren criteria zoals de mogelijkheid om Nederlandstalige audio te transcriberen ‚Äì niet alle publiek beschikbare modellen kunnen dit ‚Äì en het bijhorende prijskaartje. In de studie werden uiteindelijk SeamlessM4T-v2 (van Meta), Whisper Large-v3 (OpenAI), Nova-2 (Deepgram), Google STT (Google) en Chirp (Google) met elkaar vergeleken. Dit zijn allemaal zogenaamde multilinguale (meertalige) modellen. Daarom is het belangrijk om te weten op basis van welke (Nederlandstalige) data deze modellen getraind werden. Volgens de beschikbare documentatie werd Seamless getraind op 6363 uren en Whisper 2077 uren aan Nederlandse tekst (geen Vlaams). Voor de andere modellen is geen precieze informatie over specifieke Nederlandstalige training van de modellen bekend.\n\n\nResultaten\n\nBovenstaande grafieken tonen dat de onderzochte spraakmodellen niet goed presteren op Nederlandstalige audiofragmenten, met een gemiddelde WER tot 77%! Voorts valt op dat de WER voor Vlaamse fragmenten consistent hoger ligt dan voor Nederlandse (uit Nederland), met verschillen tussen de 5 en 12 procentpunten. Dat betekent dus dat de modellen slechter presteren op Vlaamse audio. Als we verder inzoomen op regionale verschillen in Vlaanderen, stellen we vast dat de geteste spraakherkenningsmodellen het meeste moeite hebben met West-Vlaamse fragmenten. Het model dat het beste presteert voor Vlaamse spraakherkenning is Chirp van Google.\nWat betekenen deze resultaten nu concreet voor de bruikbaarheid van huidige state-of-the-art spraakherkenningsmodellen voor het Vlaams? In ieder geval maakt dit onderzoek duidelijk dat de technologie nog geen echt bruikbare herkenning van Vlaamse (tussen)taal oplevert. Betekent dit dat we gewoon moeten wachten tot er betere modellen op de markt komen? Niet noodzakelijk. Zo is het ‚Äì zonder te technisch te willen worden ‚Äì mogelijk om bestaande taalmodellen te verbeteren voor specifieke doeleinden zonder ze volledig opnieuw te moeten trainen. Dit proces heet finetunen en is bijvoorbeeld mogelijk wanneer de broncode van de modellen die we willen verbeteren beschikbaar is. Van de geteste modellen in het onderzoek zijn Seamless van Meta en Whisper van OpenAI open source en dus modellen die in aanmerking komen om te finetunen. Een eerste poging om Whisper te finetunen specifiek voor de Vlaamse taal leverde alvast een substanti√´le verbetering op: de WER verminderde van 76% tot 52%. Dit is niet helemaal verrassend aangezien er voor Whisper in de trainingsfase geen gebruik gemaakt werd van Vlaamse trainingsdata. Toch valt op dat zelfs deze verbetering niet genoeg is om Chirp van Google als Nederlandse spraakherkenningskampioen van de troon te stoten."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to our Data Blog!",
    "section": "",
    "text": "Welcome to the inaugural post of the Centre for Applied Data Science at HOGENT. We are a dedicated team of data experts committed to advancing the field of data science through innovative research, education, and application. Our journey into the digital expanse is fuelled by a passion for uncovering insights and fostering knowledge that drives progress and transformation across various domains.\nAt the heart of our centre lies a profound appreciation for the power of data. In a world increasingly driven by information, we see endless opportunities to leverage data in ways that are both groundbreaking and beneficial to society. Our blog serves as a platform to share our discoveries, challenges, and the lessons we learn along the way.\nAs we embark on this exciting journey, we invite you to join us. Whether you‚Äôre a data science professional, a student eager to learn, or simply curious about the impact of data in our world, our blog aims to provide valuable content that informs, inspires, and ignites discussion.\nThrough our posts, we‚Äôll delve into all things data-related, from the latest trends and technologies to in-depth analyses and case studies. We‚Äôre here to share our expertise, explore new frontiers, and build a community of like-minded individuals passionate about harnessing the potential of data.\nSo, welcome aboard. We‚Äôre thrilled to have you with us and can‚Äôt wait to see where this journey takes us together."
  },
  {
    "objectID": "posts/datasetdocumentatie/index.html",
    "href": "posts/datasetdocumentatie/index.html",
    "title": "Praktische datasetdocumentatie voor praktijkgericht onderzoek",
    "section": "",
    "text": "Datasetdocumentatie is een van de meest onderschatte maar meest impactvolle onderdelen van Research Data Management (RDM). Goed gedocumenteerde data besparen tijd, voorkomen fouten en verhogen de herbruikbaarheid, betrouwbaarheid en impact van praktijkgericht onderzoek. Goede documentatie maakt het verschil tussen een dataset die slechts √©√©n project dient en een dataset die bruikbaar blijft voor andere onderzoekers, studenten, partners of beleidsmakers.\n\nBouw een logische datasetstructuur\nEen goede datasetdocumentatie start met een duidelijke, consistente structuur van mappen en bestanden. Dit lijkt eenvoudig, maar net hier gaat in veel projecten waardevolle tijd verloren.\nKenmerken van een goede structuur:\n\neenvoudig te begrijpen\ndoor iedereen in het project te volgen\nlogisch opgebouwd\nconsistent toegepast\nmeteen duidelijk voor externen\n\n\nAanbevolen mappenstructuur\n/projectnaam/\n‚îú‚îÄ‚îÄ data_raw/               # Ongewijzigde ruwe data\n‚îú‚îÄ‚îÄ data_clean/             # Opgeschoonde en gestructureerde data\n‚îú‚îÄ‚îÄ data_analysis/          # Afgeleide tabellen, statistische output\n‚îú‚îÄ‚îÄ docs/                   # README, codeboek, protocollen\n‚îú‚îÄ‚îÄ scripts/                # R-, Python-, SPSS-, Stata- of SQL-code\n‚îú‚îÄ‚îÄ metadata/               # Formele metadata\n‚îî‚îÄ‚îÄ results/                # Tabellen, figuren, rapporten\nVoorbeeld uit praktijkgericht onderzoek\nIn een project rond voedingsgewoonten bij leerlingen:\n\ndata_raw/: CSV-export uit QuesitionPro\ndata_clean/: opgeschoonde dataset zonder vrije tekstvelden\ndocs/: codeboek met variabelen (bv. FRUIT_FREQ)\nscripts/: R-script 01_cleaning.R met datacleaning\n\n\n\n\nSchrijf een duidelijke en volledige README\nEen README is het eerste document dat een lezer gebruikt om te begrijpen hoe het project en de dataset in elkaar zitten. Zonder README wordt een dataset snel ‚Äúverweesd‚Äù, zeker wanneer projectleden wisselen of wanneer je maanden later opnieuw naar dezelfde bestanden kijkt.\nMinimale inhoud van een README\n\ntitel van het project\ndoelstelling van het onderzoek\ndatum en context van dataverzameling\nbeschrijving van alle belangrijke bestanden\nuitleg van mappenstructuur\ndefinities van afkortingen en termen\nsoftwarevereisten\ncontactgegevens\nlicentie en toegangsvoorwaarden\n\nVoorbeeld\n# README ‚Äì Re-integratiecoaching 2024\n\nDit project onderzoekt de impact van een vernieuwde re-integratieaanpak\nin stedelijke diensten. Data werden verzameld via semi-gestructureerde\ninterviews (n=34) en een survey (n=112).\n\nMappen: \n- data_raw/interviews_audio/ (mp3 ‚Äì niet gedeeld)\n- data_clean/interviews_coded.csv (ATLAS.ti export)\n- data_clean/survey_clean.csv (anoniem)\n- docs/codeboek_survey.xlsx \n- scripts/01_transcript_cleaning.R\n- results/rapport_reintegratie_2024.pdf\n\nAfkortingen:\n- RIP = Re-integratieproces\n- SCO = Sociale Competentie Ontwikkeling\n\nContact: onderzoeksteam@hogent.be\nLicentie: CC-BY-NC 4.0\n\n\nGebruik een codeboek voor kwantitatieve data\nEen codeboek bevat de volledige beschrijving van elke variabele in de dataset. Dit maakt analyses reproduceerbaar en voorkomt interpretatiefouten.\nWat moet in een codeboek staan?\n\nvariabelenaam beschrijving\nmogelijke waarden\ndatatype\neenheid (indien van toepassing)\nhoe missing values worden aangeduid\neventuele transformaties\n\nVoorbeeld\n\n\n\n\n\n\n\n\n\n\nVariabele\nBeschrijving\nWaarden\nType\nEenheid\n\n\nBF_PERCENT\nLichaamsvetpercentage via BIA\n0‚Äì80\nnumeriek\n%\n\n\nACTIVITY_LEVEL\nZelfgerapporteerde fysieke activiteit\n1=laag, 2=middelmatig, 3=hoog\ninteger\nn.v.t\n\n\n\n\n\nGebruik een datadictionary voor complexere datasets\nEen datadictionary is uitgebreider dan een codeboek en is noodzakelijk wanneer een dataset bestaat uit meerdere tabellen, relationele structuren of data afkomstig uit meetapparatuur.\nVoorbeeld\nVeldproef met waterkwaliteitssensoren:\n\nsensor_id verwijst naar tabel sensors\nmetadata per sensor: locatie, diepte, type, kalibratiedatum\nmeasurement_id verwijst naar tabel measurements\nregistratiefrequentie: om de 5 minuten\nkwaliteitscontrole: waarden &gt;3 SD automatisch gemarkeerd\n\n\n\nVoeg eenvoudige metadata toe\nMetadata zijn letterlijk ‚Äúdata over data‚Äù: gestructureerde informatie die je dataset beschrijft, toelicht en annoteert. Waar een README vooral voor mensen bedoeld is, zijn metadata machine-leesbare vormen van documentatie. Ze spelen een cruciale rol in het toepassen van de FAIR-principes: Findable, Accessible, Interoperable en Reusable. Met goede metadata kan een dataset online gevonden worden, correct ge√Ønterpreteerd worden en kan eenduidig worden aangegeven onder welke voorwaarden ze toegankelijk is of hergebruikt mag worden (bijv. toegangsrechten, licentie).\nMetadata kunnen op verschillende manieren worden opgeslagen:\n\ningebed in de data zelf (bijv. in een netCDF-bestand of een JPEG-header)\nals een apart metadata-bestand (bijv. metadata.json, dataset.xml)\nvia formulieren of templates in een datamanagementsysteem\nautomatisch gegenereerd bij deponering in een repository.\n\nMetadata-schema‚Äôs\nWanneer je data deponeert in een repository, moet je metadata aanleveren volgens het metadata-schema dat die repository gebruikt. Dit gebeurt doorgaans via een webformulier of een vooropgesteld template. Het is belangrijk om tijdig de submission guidelines van de repository te bekijken, zodat je weet welke informatie je tijdens het onderzoek moet verzamelen.\nEen metadata-schema definieert een vaste set velden die moeten worden ingevuld. Sommige schema‚Äôs zijn breed toepasbaar, andere zijn specifiek ontwikkeld voor bepaalde onderzoeksdomeinen. Voorbeelden:\n\nAlgemene standaarden\n\nDublin Core: breed toepasbare set metadata-elementen\nDataCite: veel gebruikt voor datasets met een DOI (Digital Object Identifier)\n\nDomeinspecifieke standaarden\n\nDDI (Data Documentation Initiative): sociaal-, gedrags-, economisch en gezondheidswetenschappelijk onderzoek\nEML (Ecological Metadata Language): ecologie en biodiversiteitsdata\n\n\nWaar vind je geschikte metadata-standaarden?\nOnderzoekers kunnen geschikte schema‚Äôs en richtlijnen vinden via:\n\nmetadata-richtlijnen van de repository waar je dataset terechtkomt (bv. Zenodo)\nDataCite Metadata Schema\nDublin Core Metadata Schema\nMetadata Standards Catalog\nDCC: Overview of Disciplinary Metadata\nFAIRsharing.org\n\nDeze bronnen geven een overzicht van beschikbare standaarden per discipline en helpen je kiezen welke metadata-velden voor jouw dataset relevant zijn. Wil je de dataset echt FAIR maken, begin dan al tijdens de onderzoeksopzet met het verzamelen van de metadata die later in het schema nodig zijn.\n\n\nDocumenteer GDPR- en ethiekprocedures\nIn praktijkgericht onderzoek met mensen is het documenteren van GDPR- en ethische aspecten een essentieel onderdeel van datasetdocumentatie. Het gaat niet alleen om ‚Äújuridische verplichtingen‚Äù, maar ook om het waarborgen van betrouwbare en integere onderzoeksresultaten. Voor vragen of concrete toepassing in je project vind je meer informatie op de pagina Ethiek en wetenschappelijke integriteit op het HOGENT-intranet.\n\nGDPR en register van verwerkingsactiviteiten\nDe Algemene Verordening Gegevensbescherming (AVG, of GDPR) heeft als doel de bescherming van betrokkenen bij de verwerking van hun persoonsgegevens, zodat hun fundamentele rechten op privacy en gegevensbescherming worden gegarandeerd. Alle verwerkingen van persoonsgegevens moeten gedocumenteerd worden in een register van verwerkingsactiviteiten, als onderdeel van de verantwoordingsplicht. Dit sluit aan bij de HOGENT-gedragscode voor het verwerken van persoonsgegevens en vertrouwelijke informatie.\nHOGENT draagt als instelling de verantwoordelijkheid voor de rechtmatige en veilige verwerking van persoonsgegevens en voor de naleving van de GDPR. Deze verantwoordelijkheid wordt gedeeld met de voor het onderzoek verantwoordelijke onderzoeker(s).\nConcreet betekent dit:\n\nAlle onderzoeksprojecten die persoonsgegevens verwerken, moeten worden geregistreerd in DMPonline.be.\nDe registratie gebeurt aan het begin van elk nieuw onderzoeksproject, dus v√≥√≥r de start van de eigenlijke dataverzameling.\nDe geregistreerde verwerkingsactiviteiten moeten gedurende de looptijd van het project up-to-date gehouden worden.\nOok reeds lopende projecten moeten alsnog geregistreerd worden.\n\nDe template in DMPonline.be geeft onderzoekers een overzicht van de informatie die nodig is om te voldoen aan de GDPR en wijst op mogelijke risico‚Äôs v√≥√≥r, tijdens en na het onderzoeksproject. Zorgvuldige en rechtmatige verwerking van persoonsgegevens is een voorwaarde voor betrouwbare wetenschappelijke resultaten.\nMeer informatie vind je via het HOGENT-intranet en DMPonline.be.\n\n\nInformed consent\nWanneer je met personen werkt, moeten respondenten duidelijk ge√Ønformeerd worden over:\n\nde doelstelling van het onderzoek\nwelke gegevens verzameld worden\nwat je met die gegevens zal doen\nwaar en hoe lang de gegevens bewaard worden\nof en hoe de data gedeeld of hergebruikt zullen worden\n\nDit gebeurt via een informed consent. Als je onderzoeksdata wil delen met derden of later opnieuw wil gebruiken (bijvoorbeeld in vervolgonderzoek), moet je daar expliciet toestemming voor vragen. Dit leg je vast in een informed consent-formulier dat door elke deelnemer wordt ondertekend.\nOok de keuzes rond anonimisering/pseudonimisering en de rechten van deelnemers (inzage, recht op intrekking, enz.) worden hier duidelijk in beschreven.\n\n\nEthische goedkeuring\nVeel onderzoeksprojecten die met mensen werken, vereisen ethische goedkeuring v√≥√≥r de start van het onderzoek. HOGENT voorziet hiervoor een beslissingsboom die helpt bepalen:\n\nof ethische goedkeuring nodig is\nwelke ethische commissie het meest geschikt is (bijvoorbeeld specifiek voor gezondheidszorg, sociaal werk, onderwijs, dierproeven, ‚Ä¶)\n\nBelangrijke aandachtspunten:\n\nEthische goedkeuring moet altijd v√≥√≥r de start van de dataverzameling aangevraagd worden.\nOok onderzoek dat niet rechtstreeks met mensen werkt kan ethische implicaties hebben (bijvoorbeeld dierproeven of gevoelige contexten) en dus goedkeuring vereisen.\nDe keuze van de commissie gebeurt op basis van de aard van het onderzoek en de doelgroep.\n\nIn je datasetdocumentatie (README, metadata, DMP) leg je vast:\n\nof en hoe GDPR van toepassing is\nin welke mate persoonsgegevens verwerkt worden\nhoe informed consent is geregeld\nwelke ethische commissie een goedkeuring heeft verleend (met referentie/nummer)\n\nDat maakt je onderzoek niet alleen juridisch correct, maar ook transparant en integere wetenschap.\n\n\n\nLeg je opslag- en versiebeheerbeleid vast\nVersiebeheer en veilige opslag bepalen de betrouwbaarheid van je onderzoek.\n3‚Äì2‚Äì1 backupregel\n\n3 kopie√´n\nop 2 verschillende media\nwaarvan 1 externe locatie\n\nAanbevolen tools\n\nOneDrive (automatische synchronisatie)\nGitHub of GitLab voor code\nversiegeschiedenis in Google Sheets of Excel\nchangelogs in dezelfde map als scripts\n\n\n\nDocumenteer alle datacleaning- en analysekoppelingen\nDocumenteer elke stap zodat analyses reproduceerbaar blijven. Dit kan via:\n\nR- of Python-scripts, SPSS syntaxen\neen tekstueel changelog\neen combinatie daarvan\n\nVoorbeeld\n2025-04-12 ‚Äì WDK\n- Outliers \\&gt;3 SD verwijderd voor 'response_time'\n- Nieuwe variabele 'normalized_time' toegevoegd\n- Log-transformatie toegepast op 'latency'\n\n\nDocumenteer de voorwaarden voor delen en licenties\nNiet alle data kunnen open gedeeld worden, maar documentatie moet duidelijk maken wat wel en niet gedeeld mag worden.\nDocumenteer:\n\nwelke datasets publiek kunnen\nwelke enkel intern beschikbaar zijn\nwelke volledig vertrouwelijk zijn\nwelke licentie wordt gebruikt (bv. CC-BY, CC-BY-SA, CC-BY-NC, MIT)\n\nVoorbeeld (AI-onderzoek)\n\ntrainingsdata ‚Üí niet publiek (rechtelijke beperkingen)\nsynthetische data ‚Üí publiek (CC-BY)\nscripts ‚Üí GitHub (MIT-licentie)\n\nWanneer je code, documentatie of datasets op GitHub beheert, kun je gebruikmaken van de **REUSE-specificatie** (Free Software Foundation Europe) om licenties correct, volledig en machine-leesbaar te dokumenteren.\nDe REUSE-tool controleert automatisch:\n\nof elk bestand een licentieheader heeft ¬†\nof alle licentie-informatie volledig en consistent is ¬†\nof je project voldoet aan de REUSE-standard voor open en herbruikbare software/data\n\nDit maakt je repository transparanter, juridisch correct en eenvoudiger te archiveren in open data repositories.\nMeer info:\n- REUSE-documentatie: https://reuse.software¬†\n- REUSE-tool (GitHub): https://github.com/fsfe/reuse-tool¬†\n\n\nConclusie\nDatasetdocumentatie is geen administratieve verplichting maar een kwaliteitsinstrument. Met:\n\neen duidelijke mappenstructuur\neen README\neen codeboek\neen datadictionary\nmetadata\nGDPR-documentatie\nchangelogs en versiebeheer\n\nzorg je ervoor dat je onderzoeksdata helder, toekomstbestendig en bruikbaar blijven, voor jezelf, voor collega-onderzoekers en voor de bredere onderzoekscommunity.\n\n\nMeer weten?\nOnderzoeksdatamanagement"
  },
  {
    "objectID": "posts/duplicate-detection/index.html",
    "href": "posts/duplicate-detection/index.html",
    "title": "Duplicate Detection",
    "section": "",
    "text": "A common problem with master data is that different records refer to the same real world entity. This can occur, for example, when a customer was registered twice (once when first contacting the company and a second time when an order was actually placed), or when a product was created twice.\nAt first sight, discovering duplicates seems easy to solve: go through all record pairs and verify whether they are the same or not. In practice, however, this is much more difficult because duplicate records are often similar but not identical. The question is then to identify these similar record pairs."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#character-based-methods",
    "href": "posts/duplicate-detection/index.html#character-based-methods",
    "title": "Duplicate Detection",
    "section": "2.1 Character-based methods",
    "text": "2.1 Character-based methods\nAs the name suggests, character-based methods work at the level of individual characters. These methods give good results when the cause of the errors is expected to be small typing errors, e.g.¬†swapping two letters or accidentally adding a small number of characters. Within this category, we recognise the following distance measures:\n\nEdit distance: this distance function is good at recognising small typographical errors.\nAffine gap distance: this distance function takes into account that once one has inserted an extra character one is likely to insert additional extra characters.\nDistance based on n-grams: this considers how many \\(n\\)-grams two strings have in common, the intuition being that similar strings have many \\(n\\)-grams in common but strings that differ a lot do not.\n\nBelow we give a brief explanation of each of these 3 distance measures, along with additional references where you can find more detail.\n\n2.1.1 Edit distance\nThe edit or Levenshtein distance is the minimum number of substitutions, insertions and deletions required to convert two strings into each other. For example, the Levenshtein distance between ‚ÄúCADS‚Äù and ‚ÄúLAST‚Äù is equal to three.\nCADS (substitute C by L) -&gt; LADS (delete D) -&gt; LAS (insert T) -&gt; LAST\n\n\n2.1.2 Affine gap distance\nWhen strings were abbreviated or shortened, the edit distance sometimes shows a large value even though they are about the same entity. An example would be the strings ‚ÄúJohn R. Smith‚Äù and ‚ÄúJonathan Richard Smith‚Äù. With the affine gap distance, one adjusts the Levenshtein distance by introducing two additional operations, namely ‚Äúopening‚Äù a hole and ‚Äúexpanding‚Äù a hole, where typically opening a hole has a greater cost (i.e.¬†will give rise to a greater distance) than expanding a hole. The reasoning is that once one has introduced an additional first character one might add several more.\nBy way of example we show the affine gap distance between the words ‚ÄúBoulevard‚Äù and ‚ÄúBlvd‚Äù.\nBoulevard (1 deletion) =&gt; Bulevard\nBulevard (0.5 subsequent deletetion) =&gt; \"Blevard\"\nBlevard (1 deletion) =&gt; Blvard\nBlvard (1 deletion) =&gt; Blvrd\nBlvrd (0.5 subsequent deletion) =&gt; Blvd \nThe affine gap distance between ‚ÄúBoulevard‚Äù and ‚ÄúBlvd‚Äù is 4 whereas the regular edit distance would yield a value of 5.\n\n\n2.1.3 Distance based on n-grams\nAn \\(n\\)-gram of a string is nothing but a sequence of \\(n\\) characters of that string. E.g. all 2-grams of ‚Äúbooks‚Äù are ‚Äúbo‚Äù, ‚Äúoo‚Äù, ‚Äúok‚Äù and ‚Äúks‚Äù. The 2-grams of ‚Äúboots‚Äù are ‚Äúbo‚Äù, ‚Äúoo‚Äù, ‚Äúot‚Äù and ‚Äúts‚Äù.\nTo calculate the distance based on 2-grams between two words, one looks at all 2-grams occurring in at least one of the words and considers the absolute value of the difference between the number of occurrences in the two words. For the example above this becomes:\n\nTable 1: 2-grams and their occurrences in the words ‚Äúbooks‚Äù and ‚Äúboots‚Äù\n\n\n2-gram\nbooks\nboots\ndifference\n\n\n\n\nbo\n1\n1\n0\n\n\noo\n1\n1\n0\n\n\nok\n1\n0\n1\n\n\nks\n1\n0\n1\n\n\not\n0\n1\n1\n\n\nts\n0\n1\n1\n\n\n\nHence, the distance based on 2-grams between ‚Äúbooks‚Äù and ‚Äúboots‚Äù is 4."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#token-based-methods",
    "href": "posts/duplicate-detection/index.html#token-based-methods",
    "title": "Duplicate Detection",
    "section": "2.2 Token-based methods",
    "text": "2.2 Token-based methods\nWhen words are swapped places in two strings, character-based methods will typically attribute a large distance (or low similarity) to these two strings. Methods based on ‚Äútokens‚Äù attempt to address this.\nWe list some token-based methods below:\n\nMethod based on ‚Äúatomic strings‚Äù. This method will typically work well when certain words are sometimes abbreviated.\nA method combining n-grams with ‚Äútf.idf‚Äù\n\n\n2.2.1 Method based on ‚Äúatomic strings‚Äù\nIn this context, an ‚Äúatomic string‚Äù refers to a sequence of alphanumeric characters bounded by other characters. We say that two ‚Äúatomic strings‚Äù produce a match when they are equal or when one of them is a prefix of the other. E.g. ‚ÄúUniv‚Äù and ‚ÄúUniversity‚Äù match because the former is a prefix of the latter.\nIn this method, the similarity between two strings A and B is defined as the number of atomic strings of A that yield a match to an atomic string of B divided by the average number of atomic strings in strings A and B.\nBy way of example, suppose that:\n\nstring A equals ‚ÄúComput. Sci. & Eng. Dept., University of California, San Diego‚Äù\nand that string B equals ‚ÄúDepartment of Computer Science, Univ. Calif., San Diego‚Äù\n\nThe atomic strings of A and B are: - for string A: ‚ÄúComput‚Äù, ‚ÄúSci‚Äù, ‚ÄúEng‚Äù, ‚ÄúDept‚Äù, ‚ÄúUniversity‚Äù, ‚Äúof‚Äù, ‚ÄúCalifornia‚Äù, ‚ÄúSan‚Äù, ‚ÄúDiego‚Äù - for string B: ‚ÄúDepartment‚Äù, ‚Äúof‚Äù, ‚ÄúComputer‚Äù, ‚ÄúScience‚Äù, ‚ÄúUniv‚Äù, ‚ÄúCalif‚Äù, ‚ÄúSan‚Äù, ‚ÄúDiego‚Äù\nThe following atomic strings of A match with an atomic string of B:\n\n‚ÄúComput‚Äù matches with ‚ÄúComputer‚Äù\n‚ÄúSci‚Äù matches with ‚ÄúScience‚Äù\n‚ÄúUniversity‚Äù matches with ‚ÄúUniv‚Äù\n‚Äúof‚Äù matches with ‚Äúof‚Äù\n‚ÄúCalifornia‚Äù matches with ‚ÄúCalif‚Äù\n‚ÄúSan‚Äù matches with ‚ÄúSan‚Äù\n‚ÄúDiego‚Äù matches with ‚ÄúDiego‚Äù\n\nConsequently, the number of atomic strings of A that match an atomic string of B equals 7. On average, strings A and B have \\((9 + 8)/2 = 8.5\\) atomic strings. Consequently, the similarity, based on this method of atomic strings, between A and B is: \\(7/8.5 = 0.82\\).\nIf one were to remove the stop word ‚Äúof‚Äù the similarity would become \\(6/7.5 = 0.8\\).\n\n\n2.2.2 Method combining n-grams with ‚Äútf.idf‚Äù\ntf.idf, which stands for ‚Äúterm frequency, inverse document frequency‚Äù is a number indicating how important a word is to the content of a document compared to a collection of documents. Terms that occur frequently in a document have a high ‚Äúterm frequency‚Äù. However, if a term occurs in many documents then it also has a high ‚Äúdocument frequency‚Äù. To determine the tf.idf of a word in a document, the ‚Äúterm frequency‚Äù is divided by the ‚Äúdocument frequency‚Äù. So one only gets a high ‚Äútf.idf‚Äù for word in a document if this word occurs frequently in this document and does not occur in many other documents.\nAs a next step, one can put all terms (words) appearing in all documents in a certain order (e.g.¬†alphabetically). A single document can then be summarised by a (very long) list of numbers, each number being the tf.idf of a given term.\nComparing long lists of numbers (i.e.¬†of vectors) is a problem that has been studied extensively. One of the typical ways of comparing such lists of numbers is the so-called cosine similarity. When two lists of numbers are exactly equal then it gives a value equal to +1, when they are exactly opposite the value is equal to -1. Thus, if one wants to identify similar documents, one looks for documents for which the cosine similarity of their tf.idf lists is ‚Äúlarge‚Äù.\nWithin the context of database tables, there are few fields that contain enough different words to calculate the tf.idf in a meaningful way. The trick now is to apply the above procedure to the \\(n\\)-grams of the fields. Hence, to apply this technique to compare two strings (in a column) we also need the contents of all (other) strings in the same column."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#phonetic-methods",
    "href": "posts/duplicate-detection/index.html#phonetic-methods",
    "title": "Duplicate Detection",
    "section": "2.3 Phonetic Methods",
    "text": "2.3 Phonetic Methods\nThese methods are typically highly language-dependent. Here, words are compared based on their pronunciation. Words with similar pronunciation are assigned higher similarity."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#methods-based-on-word-semantics",
    "href": "posts/duplicate-detection/index.html#methods-based-on-word-semantics",
    "title": "Duplicate Detection",
    "section": "2.4 Methods Based on Word Semantics",
    "text": "2.4 Methods Based on Word Semantics\nBut what if we compare words that are not syntactically similar at all, but which mean the same thing? For example, the words ‚ÄúCar‚Äù and ‚ÄúAutomobile‚Äù are synonyms of each other, but would have little or no similarity with the methods discussed above. We can determine the similarity of words by comparing their word vectors, or word embeddings. Word embeddings can be generated with an algorithm like word2vec: As the name suggests, word2vec represents each individual word with a list of numbers, called a vector. The vectors are carefully constructed so that a simple mathematical function (the cosine equality between the vectors) indicates the degree of semantic similarity between the words represented by those vectors. In essence, a word is transformed into a sequence of some 300 numbers and these sequences of numbers will be very similar when comparing words that have the same meaning.\nThese methods will work well when the words in a column are common words; when it comes to a specific jargon then the words may not be recognised or the vectors with which they are represented will not necessarily show the right behaviour in terms of similarity."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#limiting-the-number-of-record-comparisons",
    "href": "posts/duplicate-detection/index.html#limiting-the-number-of-record-comparisons",
    "title": "Duplicate Detection",
    "section": "3.1 Limiting the Number of Record Comparisons",
    "text": "3.1 Limiting the Number of Record Comparisons\nWhen running algorithms, it is important that they finish executing within a reasonable time. This is where there can be a catch with duplicate detection. If one has e.g.¬†\\(1000\\) records then the number of pairs is about 500,000! (The exact number is \\(1000 \\times 999 / 2 = 499,500\\) but \\(500,000\\) is obviously easier to work with). For a dataset with \\(10,000\\) records, the number of pairs is already about \\(50,000,000\\)! Even for fast and powerful computers, this can quickly become a problem. If one wants to express this technically, one says that the number of pairs is of order \\(n^2\\), where \\(n\\) represents the number of records.\nIn certain cases, however, one may have domain knowledge that allows one to deduce that records that differ in a particular column (or in the initial letters of a column value) are most likely not duplicates. One can then use this to dramatically reduce the number of records to be compared.\nBy way of example, suppose one has a customer list and one has stored the gender of the customer. To keep the example simple, we assume that there are only two possible values, i.e.¬†‚ÄòM‚Äô and ‚ÄòF‚Äô. While it is possible that the gender was noted incorrectly and thus duplicates occur between ‚ÄòM‚Äô and ‚ÄòF‚Äô this seems unlikely. Now if we assume that there are \\(1000\\) customers of which \\(500\\) are ‚ÄòM‚Äô and \\(500\\) are ‚ÄòF‚Äô, and we compare only within ‚ÄòM‚Äô and within ‚ÄòF‚Äô then the number of records to be compared is about \\(125,000\\) (for ‚ÄòM‚Äô) and \\(125,000\\) (for ‚ÄòV‚Äô). Together this is \\(250,000\\), and thus about half of what the number of records to compare would be without this division.\nIf we can partition even more, the gains become even greater. Suppose there is a certain column with \\(10\\) different values that occur \\(100\\) times each (i.e.¬†there are still 1000 records) and it is very unlikely that any two records are duplicates when they have a different value for this column. If, as already mentioned, we assume that each value occurs \\(100\\) times then the number of records to compare is roughly equal to \\(10 \\times 5000\\) which is equal to \\(50,000\\). Compare this with the \\(500,000\\) records we have to compare without this division.\nThe technical name for this partitioning is blocking, and it is a crucial technique for making duplicate detection scalable."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CADS Data Blog",
    "section": "",
    "text": "Praktische datasetdocumentatie voor praktijkgericht onderzoek\n\n\n\n\n\n\ndatasetdocumentatie\n\n\nresearch data management\n\n\n\n\n\n\n\n\n\nNov 8, 2025\n\n\nWillem De Keyzer\n\n\n\n\n\n\n\n\n\n\n\n\nAnonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro\n\n\n\n\n\n\nonderzoek\n\n\nprivacy\n\n\nsurveys\n\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nWillem De Keyzer\n\n\n\n\n\n\n\n\n\n\n\n\nSpraakherkenning van Vlaamse dialecten tot algemeen Nederlands\n\n\nDer is nog vree veel werk aan de winkel!\n\n\n\nspraakherkenning\n\n\nAI\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nJens Coetsiers, Lena De Mol, Stijn Lievens en Jan Claes\n\n\n\n\n\n\n\n\n\n\n\n\nDuplicate Detection\n\n\n\n\n\n\ndata cleaning\n\n\nduplicate detection\n\n\nfuzzy matching\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nStijn Lievens\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Qualtrics survey results into Power BI\n\n\n\n\n\n\ndata engineering\n\n\nbusiness analytics\n\n\nsurveys\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nWillem De Keyzer\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to our Data Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nCADS\n\n\n\n\n\n\nNo matching items"
  }
]