---
title: "Praktische datasetdocumentatie voor praktijkgericht onderzoek"
author: "Willem De Keyzer"
date: 2025-11-08
categories: [datasetdocumentatie, research data management]
image: "datadocumentatie.png"
toc: true
draft: false
number-sections: false
editor: 
  markdown: 
    wrap: 72
---

Datasetdocumentatie is een van de meest onderschatte maar meest
impactvolle onderdelen van Research Data Management (RDM). Goed
gedocumenteerde data besparen tijd, voorkomen fouten en verhogen de
herbruikbaarheid, betrouwbaarheid en impact van praktijkgericht
onderzoek. Goede documentatie maakt het verschil tussen een dataset die
slechts één project dient en een dataset die bruikbaar blijft voor
andere onderzoekers, studenten, partners of beleidsmakers.

### Bouw een logische datasetstructuur

Een goede datasetdocumentatie start met een duidelijke, consistente
structuur van mappen en bestanden. Dit lijkt eenvoudig, maar net hier
gaat in veel projecten waardevolle tijd verloren.

Kenmerken van een goede structuur:

-   eenvoudig te begrijpen

-   door iedereen in het project te volgen

-   logisch opgebouwd

-   consistent toegepast

-   meteen duidelijk voor externen

#### Aanbevolen mappenstructuur

``` text
/projectnaam/
├── data_raw/               # Ongewijzigde ruwe data
├── data_clean/             # Opgeschoonde en gestructureerde data
├── data_analysis/          # Afgeleide tabellen, statistische output
├── docs/                   # README, codeboek, protocollen
├── scripts/                # R-, Python-, SPSS-, Stata- of SQL-code
├── metadata/               # Formele metadata
└── results/                # Tabellen, figuren, rapporten
```

**Voorbeeld uit praktijkgericht onderzoek**

In een project rond voedingsgewoonten bij leerlingen:

-   data_raw/: CSV-export uit QuesitionPro

-   data_clean/: opgeschoonde dataset zonder vrije tekstvelden

-   docs/: codeboek met variabelen (bv. FRUIT_FREQ)

-   scripts/: R-script 01_cleaning.R met datacleaning

### Schrijf een duidelijke en volledige README

Een README is het eerste document dat een lezer gebruikt om te begrijpen
hoe het project en de dataset in elkaar zitten. Zonder README wordt een
dataset snel “verweesd”, zeker wanneer projectleden wisselen of wanneer
je maanden later opnieuw naar dezelfde bestanden kijkt.

**Minimale inhoud van een README**

-   titel van het project

-   doelstelling van het onderzoek

-   datum en context van dataverzameling

-   beschrijving van alle belangrijke bestanden

-   uitleg van mappenstructuur

-   definities van afkortingen en termen

-   softwarevereisten

-   contactgegevens

-   licentie en toegangsvoorwaarden

**Voorbeeld**

``` text
# README – Re-integratiecoaching 2024

Dit project onderzoekt de impact van een vernieuwde re-integratieaanpak
in stedelijke diensten. Data werden verzameld via semi-gestructureerde
interviews (n=34) en een survey (n=112).

Mappen: 
- data_raw/interviews_audio/ (mp3 – niet gedeeld)
- data_clean/interviews_coded.csv (ATLAS.ti export)
- data_clean/survey_clean.csv (anoniem)
- docs/codeboek_survey.xlsx 
- scripts/01_transcript_cleaning.R
- results/rapport_reintegratie_2024.pdf

Afkortingen:
- RIP = Re-integratieproces
- SCO = Sociale Competentie Ontwikkeling

Contact: onderzoeksteam@hogent.be
Licentie: CC-BY-NC 4.0
```

### Gebruik een codeboek voor kwantitatieve data

Een codeboek bevat de volledige beschrijving van elke variabele in de
dataset. Dit maakt analyses reproduceerbaar en voorkomt
interpretatiefouten.

Wat moet in een codeboek staan?

-   variabelenaam beschrijving

-   mogelijke waarden

-   datatype

-   eenheid (indien van toepassing)

-   hoe missing values worden aangeduid

-   eventuele transformaties

**Voorbeeld**

|  |  |  |  |  |
|---------------|---------------|---------------|---------------|---------------|
| **Variabele** | **Beschrijving** | **Waarden** | **Type** | **Eenheid** |
| BF_PERCENT | Lichaamsvetpercentage via BIA | 0–80 | numeriek | \% |
| ACTIVITY_LEVEL | Zelfgerapporteerde fysieke activiteit | 1=laag, 2=middelmatig, 3=hoog | integer | n.v.t |

### Gebruik een datadictionary voor complexere datasets

Een datadictionary is uitgebreider dan een codeboek en is noodzakelijk
wanneer een dataset bestaat uit meerdere tabellen, relationele
structuren of data afkomstig uit meetapparatuur.

**Voorbeeld**

Veldproef met waterkwaliteitssensoren:

-   `sensor_id` verwijst naar tabel `sensors`

-   metadata per sensor: locatie, diepte, type, kalibratiedatum

-   `measurement_id` verwijst naar tabel `measurements`

-   registratiefrequentie: om de 5 minuten

-   kwaliteitscontrole: waarden \>3 SD automatisch gemarkeerd

### Voeg eenvoudige metadata toe

Metadata zijn letterlijk “data over data”: gestructureerde informatie
die je dataset beschrijft, toelicht en annoteert. Waar een README vooral
voor mensen bedoeld is, zijn metadata machine-leesbare vormen van
documentatie. Ze spelen een cruciale rol in het toepassen van de
FAIR-principes: Findable, Accessible, Interoperable en Reusable. Met
goede metadata kan een dataset online gevonden worden, correct
geïnterpreteerd worden en kan eenduidig worden aangegeven onder welke
voorwaarden ze toegankelijk is of hergebruikt mag worden (bijv.
toegangsrechten, licentie).

Metadata kunnen op verschillende manieren worden opgeslagen:

-   ingebed in de data zelf (bijv. in een netCDF-bestand of een
    JPEG-header)

-   als een apart metadata-bestand (bijv. `metadata.json`,
    `dataset.xml`)

-   via formulieren of templates in een datamanagementsysteem

-   automatisch gegenereerd bij deponering in een repository.

**Metadata-schema’s**

Wanneer je data deponeert in een repository, moet je metadata aanleveren
volgens het metadata-schema dat die repository gebruikt. Dit gebeurt
doorgaans via een webformulier of een vooropgesteld template. Het is
belangrijk om tijdig de submission guidelines van de repository te
bekijken, zodat je weet welke informatie je tijdens het onderzoek moet
verzamelen.

Een metadata-schema definieert een vaste set velden die moeten worden
ingevuld. Sommige schema’s zijn breed toepasbaar, andere zijn specifiek
ontwikkeld voor bepaalde onderzoeksdomeinen. Voorbeelden:

-   **Algemene standaarden**

    -   Dublin Core: breed toepasbare set metadata-elementen

    -   DataCite: veel gebruikt voor datasets met een DOI (Digital
        Object Identifier)

-   **Domeinspecifieke standaarden**

    -   DDI (Data Documentation Initiative): sociaal-, gedrags-,
        economisch en gezondheidswetenschappelijk onderzoek

    -   EML (Ecological Metadata Language): ecologie en
        biodiversiteitsdata

**Waar vind je geschikte metadata-standaarden?**

Onderzoekers kunnen geschikte schema’s en richtlijnen vinden via:

-   metadata-richtlijnen van de repository waar je dataset terechtkomt
    (bv. Zenodo)

-   [DataCite Metadata
    Schema](http://schema.datacite.org/meta/kernel-4.5/)

-   [Dublin Core Metadata
    Schema](https://www.dublincore.org/specifications/dublin-core/dcmi-terms/)

-   [Metadata Standards Catalog](https://rdamsc.bath.ac.uk/)

-   DCC: [Overview of Disciplinary
    Metadata](http://www.dcc.ac.uk/resources/metadata-standards)

-   [FAIRsharing.org](FAIRsharing.org)

Deze bronnen geven een overzicht van beschikbare standaarden per
discipline en helpen je kiezen welke metadata-velden voor jouw dataset
relevant zijn. Wil je de dataset echt FAIR maken, begin dan al tijdens
de onderzoeksopzet met het verzamelen van de metadata die later in het
schema nodig zijn.

### Documenteer GDPR- en ethiekprocedures

In praktijkgericht onderzoek met mensen is het documenteren van GDPR- en
ethische aspecten een essentieel onderdeel van datasetdocumentatie. Het
gaat niet alleen om “juridische verplichtingen”, maar ook om het
waarborgen van betrouwbare en integere onderzoeksresultaten. Voor vragen
of concrete toepassing in je project vind je meer informatie op de
pagina **Ethiek en wetenschappelijke integriteit** op het
HOGENT-intranet.

#### GDPR en register van verwerkingsactiviteiten

De Algemene Verordening Gegevensbescherming (AVG, of GDPR) heeft als
doel de bescherming van betrokkenen bij de verwerking van hun
persoonsgegevens, zodat hun fundamentele rechten op privacy en
gegevensbescherming worden gegarandeerd. Alle verwerkingen van
persoonsgegevens moeten gedocumenteerd worden in een register van
verwerkingsactiviteiten, als onderdeel van de verantwoordingsplicht. Dit
sluit aan bij de HOGENT-gedragscode voor het verwerken van
persoonsgegevens en vertrouwelijke informatie.

HOGENT draagt als instelling de verantwoordelijkheid voor de rechtmatige
en veilige verwerking van persoonsgegevens en voor de naleving van de
GDPR. Deze verantwoordelijkheid wordt gedeeld met de voor het onderzoek
verantwoordelijke onderzoeker(s).

Concreet betekent dit:

-   Alle onderzoeksprojecten die persoonsgegevens verwerken, moeten
    worden geregistreerd in **DMPonline.be**.

-   De registratie gebeurt aan het begin van elk nieuw
    onderzoeksproject, dus vóór de start van de eigenlijke
    dataverzameling.

-   De geregistreerde verwerkingsactiviteiten moeten gedurende de
    looptijd van het project up-to-date gehouden worden.

-   Ook reeds lopende projecten moeten alsnog geregistreerd worden.

De template in DMPonline.be geeft onderzoekers een overzicht van de
informatie die nodig is om te voldoen aan de GDPR en wijst op mogelijke
risico’s vóór, tijdens en na het onderzoeksproject. Zorgvuldige en
rechtmatige verwerking van persoonsgegevens is een voorwaarde voor
betrouwbare wetenschappelijke resultaten.

Meer informatie vind je via het HOGENT-intranet en
[DMPonline.be](https://dmponline.be).

#### Informed consent

Wanneer je met personen werkt, moeten respondenten duidelijk
geïnformeerd worden over:

-   de doelstelling van het onderzoek

-   welke gegevens verzameld worden

-   wat je met die gegevens zal doen

-   waar en hoe lang de gegevens bewaard worden

-   of en hoe de data gedeeld of hergebruikt zullen worden

Dit gebeurt via een informed consent. Als je onderzoeksdata wil delen
met derden of later opnieuw wil gebruiken (bijvoorbeeld in
vervolgonderzoek), moet je daar expliciet toestemming voor vragen. Dit
leg je vast in een informed consent-formulier dat door elke deelnemer
wordt ondertekend.

Ook de keuzes rond anonimisering/pseudonimisering en de rechten van
deelnemers (inzage, recht op intrekking, enz.) worden hier duidelijk in
beschreven.

#### Ethische goedkeuring

Veel onderzoeksprojecten die met mensen werken, vereisen ethische
goedkeuring vóór de start van het onderzoek. HOGENT voorziet hiervoor
een beslissingsboom die helpt bepalen:

-   of ethische goedkeuring nodig is

-   welke ethische commissie het meest geschikt is (bijvoorbeeld
    specifiek voor gezondheidszorg, sociaal werk, onderwijs,
    dierproeven, …)

Belangrijke aandachtspunten:

-   Ethische goedkeuring moet altijd vóór de start van de
    dataverzameling aangevraagd worden.

-   Ook onderzoek dat niet rechtstreeks met mensen werkt kan ethische
    implicaties hebben (bijvoorbeeld dierproeven of gevoelige contexten)
    en dus goedkeuring vereisen.

-   De keuze van de commissie gebeurt op basis van de aard van het
    onderzoek en de doelgroep.

In je datasetdocumentatie (README, metadata, DMP) leg je vast:

-   of en hoe GDPR van toepassing is

-   in welke mate persoonsgegevens verwerkt worden

-   hoe informed consent is geregeld

-   welke ethische commissie een goedkeuring heeft verleend (met
    referentie/nummer)

Dat maakt je onderzoek niet alleen juridisch correct, maar ook
transparant en integere wetenschap.

### Leg je opslag- en versiebeheerbeleid vast

Versiebeheer en veilige opslag bepalen de betrouwbaarheid van je
onderzoek.

**3–2–1 backupregel**

-   3 kopieën

-   op 2 verschillende media

-   waarvan 1 externe locatie

**Aanbevolen tools**

-   OneDrive (automatische synchronisatie)

-   GitHub of GitLab voor code

-   versiegeschiedenis in Google Sheets of Excel

-   changelogs in dezelfde map als scripts

### Documenteer alle datacleaning- en analysekoppelingen

Documenteer elke stap zodat analyses reproduceerbaar blijven. Dit kan
via:

-   R- of Python-scripts, SPSS syntaxen

-   een tekstueel changelog

-   een combinatie daarvan

**Voorbeeld**

``` text
2025-04-12 – WDK
- Outliers \>3 SD verwijderd voor 'response_time'
- Nieuwe variabele 'normalized_time' toegevoegd
- Log-transformatie toegepast op 'latency'
```

### Documenteer de voorwaarden voor delen en licenties

Niet alle data kunnen open gedeeld worden, maar documentatie moet
duidelijk maken wat wel en niet gedeeld mag worden.

Documenteer:

-   welke datasets publiek kunnen

-   welke enkel intern beschikbaar zijn

-   welke volledig vertrouwelijk zijn

-   welke licentie wordt gebruikt (bv. CC-BY, CC-BY-SA, CC-BY-NC, MIT)

Voorbeeld (AI-onderzoek)

-   trainingsdata → niet publiek (rechtelijke beperkingen)

-   synthetische data → publiek (CC-BY)

-   scripts → GitHub (MIT-licentie)

Wanneer je code, documentatie of datasets op GitHub beheert, kun je
gebruikmaken van de \*\*REUSE-specificatie\*\* (Free Software Foundation
Europe) om licenties correct, volledig en machine-leesbaar te
dokumenteren.

De REUSE-tool controleert automatisch:

-   of elk bestand een licentieheader heeft  

-   of alle licentie-informatie volledig en consistent is  

-   of je project voldoet aan de REUSE-standard voor open en
    herbruikbare software/data

Dit maakt je repository transparanter, juridisch correct en eenvoudiger
te archiveren in open data repositories.

Meer info:

\- REUSE-documentatie: <https://reuse.software> 

\- REUSE-tool (GitHub): <https://github.com/fsfe/reuse-tool> 

### Conclusie {.unnumbered}

Datasetdocumentatie is geen administratieve verplichting maar een
kwaliteitsinstrument. Met:

-   een duidelijke mappenstructuur

-   een README

-   een codeboek

-   een datadictionary

-   metadata

-   GDPR-documentatie

-   changelogs en versiebeheer

zorg je ervoor dat je onderzoeksdata helder, toekomstbestendig en
bruikbaar blijven, voor jezelf, voor collega-onderzoekers en voor de
bredere onderzoekscommunity.

### Meer weten? {.unnumbered}

[Onderzoeksdatamanagement](https://www.ugent.be/nl/onderzoek/openscience/datamanagement/overzicht.htm)
