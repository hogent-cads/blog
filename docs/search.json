[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to the Centre for Applied Data Science (CADS) at HOGENT University of Applied Sciences and Arts. We specialize in extracting value from data through collection, organization, analysis, and accessibility. Our team tackles diverse data-related challenges, offering expertise in Data Engineering, AI, and Geospatial Data Science. We partner with various stakeholders to create innovative, data-driven solutions. For inquiries, collaborations, or advice, contact our experts or email us at cads@hogent.be. Explore our projects and initiatives on our website or engage with us on LinkedIn and GitHub.\nWe look forward to working with you!"
  },
  {
    "objectID": "posts/qualtrics-powerbi/index.html",
    "href": "posts/qualtrics-powerbi/index.html",
    "title": "Integrating Qualtrics survey results into Power BI",
    "section": "",
    "text": "Introduction\nImagine you’re tasked with integrating Qualtrics survey results into Power BI for interactive dashboards and reports, data visualization, or combining multiple data sources. At first, this seems like a straightforward task. You dive into Power BI, expecting to find a ready-to-use connector for Qualtrics, only to realise none exists.\nYour journey takes an unexpected turn as you begin to explore alternative solutions. You discover that while some companies offer integration services, they charge exorbitant fees, up to $200 per month. This discovery propels you to look for a more cost-effective and efficient solution.\nThrough diligent research and exploration, you come across the R package {qualtRics}, a beacon of hope in your quest. This package, specifically designed for R, offers functions that allow you to fetch survey results directly from Qualtrics through an API. It’s a game-changer, offering a direct line to the data you need without the hefty price tag.\nYour next steps are clear but require careful planning and execution:\n\nSetting up an API token in Qualtrics: You’ll navigate the Qualtrics interface to generate an API token. This token is essential for authenticating your requests and securing the data transfer.\nDownloading and installing R and the {qualtRics} package: R will serve as the backbone for data manipulation and analysis. The {qualtRics} package will be the tool that bridges Qualtrics and R, enabling you to pull the data into an environment where you can manipulate it freely.\nSetting Power BI to work with R: Power BI’s versatility allows it to integrate with R scripts, a feature you plan to leverage. You’ll configure Power BI to recognise R, setting the stage for seamless data integration.\nWriting a query in Power BI to fetch data from surveys: The final piece of the puzzle involves crafting a query within Power BI that utilises R and the {qualtRics} package to fetch your desired survey data. This query will be your key to unlocking the insights hidden within your survey responses.\n\n\n\nSetting up an API Token in Qualtrics\nYour first step in this data integration journey is to obtain an API token from Qualtrics. This token acts as a key, granting you access to your survey data through the API. It ensures secure communication between Qualtrics and any external applications, like R in your case, that you might use for data analysis. Here’s how you can go about setting up an API token in Qualtrics:\n\nAccessing the Qualtrics interface: Begin by logging into your Qualtrics account. Navigate to the “Account Settings” under “My Account” at the top right corner of the page.\nFinding the API section: Within the “User Settings”, look for a section called “API”. Qualtrics continuously updates its interface, so the exact naming might vary, but you’re looking for the area where API-related settings are managed.\nGenerating the API token: In the API section, there should be an option to “Generate Token”. Clicking on this option will either generate a new token for you or take you to a screen where you can request a token.\nSecuring the token: Once your token is generated, it’s crucial to keep it secure. Treat it like a password, as it provides access to your Qualtrics data. Store it in a safe place, and avoid sharing it unnecessarily.\nGet your Data centre ID: You will also need to know your Data centre ID. You can find this under the “Account Settings” in the Qualtrics interface when you look for the section “User”.\n\nBy successfully setting up an API token in Qualtrics, you’ve taken the first significant step towards integrating your survey data with Power BI. This token will be used in subsequent steps to authenticate your data requests and ensure a seamless flow of information between Qualtrics and your analysis tools.\n\n\nDownloading and installing R and the {qualtRics} package\nAfter securing your API token from Qualtrics, the next step is to set up the tools you’ll use for fetching survey results: R and the {qualtRics} package. R is a powerful programming language used extensively in data analysis and statistical computing. The {qualtRics} package, specifically designed for R, facilitates the connection to Qualtrics, allowing you to fetch and work with your survey data seamlessly. Here’s how you can do this:\n\nDownloading R:\n\nVisit the Comprehensive R Archive Network (CRAN) at https://cran.r-project.org/.\nSelect the download link that corresponds to your operating system (Windows, Mac, or Linux).\nFollow the instructions to download and install R on your computer. The installation process is straightforward, typically involving a series of clicks through the setup wizard.\n\nRunning R and installing the {qualtRics} package: While R can be ran using a graphical interface, we will use command-line tools to install the {qualtRics} package. Here’s how you can do this:\n\nFor Windows users:\n\nOpen the Command Prompt by typing cmd in the Windows search bar.\nType R and press Enter to start an R session within the Command Prompt.\nIn the R session within the Command Prompt, type the following command and press Enter: install.packages(\"qualtRics\").\n\nFor Mac users:\n\nOpen the Terminal application (you can find it using Spotlight with Cmd + Space and then typing “Terminal”).\nType R and press Enter to start an R session within the Terminal.\nIn the R session within the Terminal, type the following command and press Enter: install.packages(\"qualtRics\").\n\n\n\nFor both Windows and Mac, after installing the {qualtRics} package, you can load it in any R session by typing library(qualtRics). This will enable you to use the package’s functions to connect to and fetch data from Qualtrics.\n\nSetting up Qualtrics API credentials in R:\n\nTo ensure R can communicate with Qualtrics using your API token, set your credentials within R. This involves using a command in the Command Prompt (Windows) or Terminal (Mac) such as in the example below (don’t mind the new lines, they are just for readability).\nReplace \"your_api_token_here\" with the actual API token you obtained from Qualtrics and \"your_data_centre_id_here\" with the appropriate name for your Qualtrics data centre ID.\n\n\n\nqualtRics::qualtrics_api_credentials(\n    api_key = \"your_api_token_here\", \n    base_url = \"your_data_centre_id_here.qualtrics.com\",\n    install = TRUE,\n    overwrite = TRUE\n    )\n\nBy completing these steps, you’ve successfully set up R and the {qualtRics} package, and you’ve configured R to communicate with Qualtrics using your API token. All the above should be done only once, and you’re now ready to use R to fetch survey data from Qualtrics and integrate it with Power BI.\n\n\nSetting Power BI to work with R\nTo unlock the power of R scripting Power BI, you first need to ensure that R is correctly referenced in your Power BI application. Open Power BI Desktop, navigate to File &gt; Options and settings &gt; Options to open the Options menu. Here, find the R scripting tab where you’ll inform Power BI about your R installation. This setup requires specifying the path to the R executable in the R home directory field. Usually, Power BI will automatically detect the R installation, but if it doesn’t, you can manually specify the path to the R executable. Once you’ve set up R in Power BI, you’re ready to start integrating your survey data with Power BI.\n\n\nWriting a query in Power BI to fetch data from surveys\nThe final step in integrating Qualtrics survey results into Power BI involves writing a query in Power BI to fetch the data. This step leverages the work you’ve done so far. Here’s how you can write and execute a query to fetch your survey data:\n\nOpen Power BI and Start a New Query:\n\nLaunch Power BI Desktop and create a new report. Navigate to the Home tab, and select Get Data. Choose More to see all data connection options.\n\nSelect R Script as the Data Source:\n\nIn the Get Data window, scroll down or search for R script, then select it and click Connect. This opens a dialog box where you can input your R script.\n\nCraft Your R Script:\n\nIn the R script input box, you’ll write an R script that utilizes the {qualtRics} package to fetch survey data from Qualtrics. R will use the API token you set up earlier to authenticate the request and fetch the data. You just need one additional parameter, and that’s the survey ID. This is the unique identifier for the survey you want to fetch data from. You can find the survey ID in the Qualtrics interface, typically in the URL when you’re viewing the survey. The survey ID is a long string of characters and numbers, and it’s unique to each survey. Here’s an example of an R script that fetches survey data using the {qualtRics} package:\n\n\n\nlibrary(qualtRics)\nmy_table &lt;- as.data.frame(\n    qualtRics::fetch_survey(surveyID = \"your_survey_ID\")\n)\n\nReplace \"your_survey_ID\" with the actual survey ID you want to fetch data from. This R script will fetch the survey data and store it in a data frame called my_table. You can then use this data frame to work with the survey data in Power BI.\n\nExecute the R Script:\n\nAfter writing your R script, click OK to execute the script. Power BI will run the R script and fetch the survey data from Qualtrics. The data will be loaded into Power BI, and you can start working with it in the Power BI interface!\n\n\nBy writing a query in Power BI to fetch data from your Qualtrics surveys, you’ve effectively bridged the gap between these powerful platforms. This integration not only streamlines your workflow but also opens up new possibilities for analyzing and visualizing survey data to inform data-driven decisions."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to our Data Blog!",
    "section": "",
    "text": "Welcome to the inaugural post of the Centre for Applied Data Science at HOGENT. We are a dedicated team of data experts committed to advancing the field of data science through innovative research, education, and application. Our journey into the digital expanse is fuelled by a passion for uncovering insights and fostering knowledge that drives progress and transformation across various domains.\nAt the heart of our centre lies a profound appreciation for the power of data. In a world increasingly driven by information, we see endless opportunities to leverage data in ways that are both groundbreaking and beneficial to society. Our blog serves as a platform to share our discoveries, challenges, and the lessons we learn along the way.\nAs we embark on this exciting journey, we invite you to join us. Whether you’re a data science professional, a student eager to learn, or simply curious about the impact of data in our world, our blog aims to provide valuable content that informs, inspires, and ignites discussion.\nThrough our posts, we’ll delve into all things data-related, from the latest trends and technologies to in-depth analyses and case studies. We’re here to share our expertise, explore new frontiers, and build a community of like-minded individuals passionate about harnessing the potential of data.\nSo, welcome aboard. We’re thrilled to have you with us and can’t wait to see where this journey takes us together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CADS Data Blog",
    "section": "",
    "text": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro\n\n\n\n\n\n\nonderzoek\n\n\nprivacy\n\n\nsurveys\n\n\n\n\n\n\n\n\n\nSep 8, 2025\n\n\nWillem De Keyzer\n\n\n\n\n\n\n\n\n\n\n\n\nSpraakherkenning van Vlaamse dialecten tot algemeen Nederlands\n\n\nDer is nog vree veel werk aan de winkel!\n\n\n\nspraakherkenning\n\n\nAI\n\n\nASR\n\n\n\n\n\n\n\n\n\nNov 6, 2024\n\n\nJens Coetsiers, Lena De Mol, Stijn Lievens en Jan Claes\n\n\n\n\n\n\n\n\n\n\n\n\nDuplicate Detection\n\n\n\n\n\n\ndata cleaning\n\n\nduplicate detection\n\n\nfuzzy matching\n\n\n\n\n\n\n\n\n\nJun 21, 2024\n\n\nStijn Lievens\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrating Qualtrics survey results into Power BI\n\n\n\n\n\n\ndata engineering\n\n\nbusiness analytics\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\nWillem De Keyzer\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to our Data Blog!\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nMar 14, 2024\n\n\nCADS\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/duplicate-detection/index.html",
    "href": "posts/duplicate-detection/index.html",
    "title": "Duplicate Detection",
    "section": "",
    "text": "A common problem with master data is that different records refer to the same real world entity. This can occur, for example, when a customer was registered twice (once when first contacting the company and a second time when an order was actually placed), or when a product was created twice.\nAt first sight, discovering duplicates seems easy to solve: go through all record pairs and verify whether they are the same or not. In practice, however, this is much more difficult because duplicate records are often similar but not identical. The question is then to identify these similar record pairs."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#character-based-methods",
    "href": "posts/duplicate-detection/index.html#character-based-methods",
    "title": "Duplicate Detection",
    "section": "2.1 Character-based methods",
    "text": "2.1 Character-based methods\nAs the name suggests, character-based methods work at the level of individual characters. These methods give good results when the cause of the errors is expected to be small typing errors, e.g. swapping two letters or accidentally adding a small number of characters. Within this category, we recognise the following distance measures:\n\nEdit distance: this distance function is good at recognising small typographical errors.\nAffine gap distance: this distance function takes into account that once one has inserted an extra character one is likely to insert additional extra characters.\nDistance based on n-grams: this considers how many \\(n\\)-grams two strings have in common, the intuition being that similar strings have many \\(n\\)-grams in common but strings that differ a lot do not.\n\nBelow we give a brief explanation of each of these 3 distance measures, along with additional references where you can find more detail.\n\n2.1.1 Edit distance\nThe edit or Levenshtein distance is the minimum number of substitutions, insertions and deletions required to convert two strings into each other. For example, the Levenshtein distance between “CADS” and “LAST” is equal to three.\nCADS (substitute C by L) -&gt; LADS (delete D) -&gt; LAS (insert T) -&gt; LAST\n\n\n2.1.2 Affine gap distance\nWhen strings were abbreviated or shortened, the edit distance sometimes shows a large value even though they are about the same entity. An example would be the strings “John R. Smith” and “Jonathan Richard Smith”. With the affine gap distance, one adjusts the Levenshtein distance by introducing two additional operations, namely “opening” a hole and “expanding” a hole, where typically opening a hole has a greater cost (i.e. will give rise to a greater distance) than expanding a hole. The reasoning is that once one has introduced an additional first character one might add several more.\nBy way of example we show the affine gap distance between the words “Boulevard” and “Blvd”.\nBoulevard (1 deletion) =&gt; Bulevard\nBulevard (0.5 subsequent deletetion) =&gt; \"Blevard\"\nBlevard (1 deletion) =&gt; Blvard\nBlvard (1 deletion) =&gt; Blvrd\nBlvrd (0.5 subsequent deletion) =&gt; Blvd \nThe affine gap distance between “Boulevard” and “Blvd” is 4 whereas the regular edit distance would yield a value of 5.\n\n\n2.1.3 Distance based on n-grams\nAn \\(n\\)-gram of a string is nothing but a sequence of \\(n\\) characters of that string. E.g. all 2-grams of “books” are “bo”, “oo”, “ok” and “ks”. The 2-grams of “boots” are “bo”, “oo”, “ot” and “ts”.\nTo calculate the distance based on 2-grams between two words, one looks at all 2-grams occurring in at least one of the words and considers the absolute value of the difference between the number of occurrences in the two words. For the example above this becomes:\n\nTable 1: 2-grams and their occurrences in the words “books” and “boots”\n\n\n2-gram\nbooks\nboots\ndifference\n\n\n\n\nbo\n1\n1\n0\n\n\noo\n1\n1\n0\n\n\nok\n1\n0\n1\n\n\nks\n1\n0\n1\n\n\not\n0\n1\n1\n\n\nts\n0\n1\n1\n\n\n\nHence, the distance based on 2-grams between “books” and “boots” is 4."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#token-based-methods",
    "href": "posts/duplicate-detection/index.html#token-based-methods",
    "title": "Duplicate Detection",
    "section": "2.2 Token-based methods",
    "text": "2.2 Token-based methods\nWhen words are swapped places in two strings, character-based methods will typically attribute a large distance (or low similarity) to these two strings. Methods based on “tokens” attempt to address this.\nWe list some token-based methods below:\n\nMethod based on “atomic strings”. This method will typically work well when certain words are sometimes abbreviated.\nA method combining n-grams with “tf.idf”\n\n\n2.2.1 Method based on “atomic strings”\nIn this context, an “atomic string” refers to a sequence of alphanumeric characters bounded by other characters. We say that two “atomic strings” produce a match when they are equal or when one of them is a prefix of the other. E.g. “Univ” and “University” match because the former is a prefix of the latter.\nIn this method, the similarity between two strings A and B is defined as the number of atomic strings of A that yield a match to an atomic string of B divided by the average number of atomic strings in strings A and B.\nBy way of example, suppose that:\n\nstring A equals “Comput. Sci. & Eng. Dept., University of California, San Diego”\nand that string B equals “Department of Computer Science, Univ. Calif., San Diego”\n\nThe atomic strings of A and B are: - for string A: “Comput”, “Sci”, “Eng”, “Dept”, “University”, “of”, “California”, “San”, “Diego” - for string B: “Department”, “of”, “Computer”, “Science”, “Univ”, “Calif”, “San”, “Diego”\nThe following atomic strings of A match with an atomic string of B:\n\n“Comput” matches with “Computer”\n“Sci” matches with “Science”\n“University” matches with “Univ”\n“of” matches with “of”\n“California” matches with “Calif”\n“San” matches with “San”\n“Diego” matches with “Diego”\n\nConsequently, the number of atomic strings of A that match an atomic string of B equals 7. On average, strings A and B have \\((9 + 8)/2 = 8.5\\) atomic strings. Consequently, the similarity, based on this method of atomic strings, between A and B is: \\(7/8.5 = 0.82\\).\nIf one were to remove the stop word “of” the similarity would become \\(6/7.5 = 0.8\\).\n\n\n2.2.2 Method combining n-grams with “tf.idf”\ntf.idf, which stands for “term frequency, inverse document frequency” is a number indicating how important a word is to the content of a document compared to a collection of documents. Terms that occur frequently in a document have a high “term frequency”. However, if a term occurs in many documents then it also has a high “document frequency”. To determine the tf.idf of a word in a document, the “term frequency” is divided by the “document frequency”. So one only gets a high “tf.idf” for word in a document if this word occurs frequently in this document and does not occur in many other documents.\nAs a next step, one can put all terms (words) appearing in all documents in a certain order (e.g. alphabetically). A single document can then be summarised by a (very long) list of numbers, each number being the tf.idf of a given term.\nComparing long lists of numbers (i.e. of vectors) is a problem that has been studied extensively. One of the typical ways of comparing such lists of numbers is the so-called cosine similarity. When two lists of numbers are exactly equal then it gives a value equal to +1, when they are exactly opposite the value is equal to -1. Thus, if one wants to identify similar documents, one looks for documents for which the cosine similarity of their tf.idf lists is “large”.\nWithin the context of database tables, there are few fields that contain enough different words to calculate the tf.idf in a meaningful way. The trick now is to apply the above procedure to the \\(n\\)-grams of the fields. Hence, to apply this technique to compare two strings (in a column) we also need the contents of all (other) strings in the same column."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#phonetic-methods",
    "href": "posts/duplicate-detection/index.html#phonetic-methods",
    "title": "Duplicate Detection",
    "section": "2.3 Phonetic Methods",
    "text": "2.3 Phonetic Methods\nThese methods are typically highly language-dependent. Here, words are compared based on their pronunciation. Words with similar pronunciation are assigned higher similarity."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#methods-based-on-word-semantics",
    "href": "posts/duplicate-detection/index.html#methods-based-on-word-semantics",
    "title": "Duplicate Detection",
    "section": "2.4 Methods Based on Word Semantics",
    "text": "2.4 Methods Based on Word Semantics\nBut what if we compare words that are not syntactically similar at all, but which mean the same thing? For example, the words “Car” and “Automobile” are synonyms of each other, but would have little or no similarity with the methods discussed above. We can determine the similarity of words by comparing their word vectors, or word embeddings. Word embeddings can be generated with an algorithm like word2vec: As the name suggests, word2vec represents each individual word with a list of numbers, called a vector. The vectors are carefully constructed so that a simple mathematical function (the cosine equality between the vectors) indicates the degree of semantic similarity between the words represented by those vectors. In essence, a word is transformed into a sequence of some 300 numbers and these sequences of numbers will be very similar when comparing words that have the same meaning.\nThese methods will work well when the words in a column are common words; when it comes to a specific jargon then the words may not be recognised or the vectors with which they are represented will not necessarily show the right behaviour in terms of similarity."
  },
  {
    "objectID": "posts/duplicate-detection/index.html#limiting-the-number-of-record-comparisons",
    "href": "posts/duplicate-detection/index.html#limiting-the-number-of-record-comparisons",
    "title": "Duplicate Detection",
    "section": "3.1 Limiting the Number of Record Comparisons",
    "text": "3.1 Limiting the Number of Record Comparisons\nWhen running algorithms, it is important that they finish executing within a reasonable time. This is where there can be a catch with duplicate detection. If one has e.g. \\(1000\\) records then the number of pairs is about 500,000! (The exact number is \\(1000 \\times 999 / 2 = 499,500\\) but \\(500,000\\) is obviously easier to work with). For a dataset with \\(10,000\\) records, the number of pairs is already about \\(50,000,000\\)! Even for fast and powerful computers, this can quickly become a problem. If one wants to express this technically, one says that the number of pairs is of order \\(n^2\\), where \\(n\\) represents the number of records.\nIn certain cases, however, one may have domain knowledge that allows one to deduce that records that differ in a particular column (or in the initial letters of a column value) are most likely not duplicates. One can then use this to dramatically reduce the number of records to be compared.\nBy way of example, suppose one has a customer list and one has stored the gender of the customer. To keep the example simple, we assume that there are only two possible values, i.e. ‘M’ and ‘F’. While it is possible that the gender was noted incorrectly and thus duplicates occur between ‘M’ and ‘F’ this seems unlikely. Now if we assume that there are \\(1000\\) customers of which \\(500\\) are ‘M’ and \\(500\\) are ‘F’, and we compare only within ‘M’ and within ‘F’ then the number of records to be compared is about \\(125,000\\) (for ‘M’) and \\(125,000\\) (for ‘V’). Together this is \\(250,000\\), and thus about half of what the number of records to compare would be without this division.\nIf we can partition even more, the gains become even greater. Suppose there is a certain column with \\(10\\) different values that occur \\(100\\) times each (i.e. there are still 1000 records) and it is very unlikely that any two records are duplicates when they have a different value for this column. If, as already mentioned, we assume that each value occurs \\(100\\) times then the number of records to compare is roughly equal to \\(10 \\times 5000\\) which is equal to \\(50,000\\). Compare this with the \\(500,000\\) records we have to compare without this division.\nThe technical name for this partitioning is blocking, and it is a crucial technique for making duplicate detection scalable."
  },
  {
    "objectID": "posts/vlaamse-spraakherkenning/index.html",
    "href": "posts/vlaamse-spraakherkenning/index.html",
    "title": "Spraakherkenning van Vlaamse dialecten tot algemeen Nederlands",
    "section": "",
    "text": "Inleiding\nSpraakherkenning is cruciaal voor toepassingen zoals automatische ondertiteling en spraakgestuurde systemen (bijvoorbeeld de voice assistent in je auto of op je smartphone). Ondanks de snelle ontwikkeling van deze technologie blijft spraakherkenning een uitdaging voor minder voorkomende talen zoals het Nederlands. Regionale variaties van de taal zorgen bovendien voor een bijkomende complexiteit. In de praktijk vertaalt zich dat in frustraties bij gebruikers wanneer ze in het Vlaams hun systemen proberen te bedienen.\nIn zijn bachelorproef onderzocht Jens Coetsiers de prestaties van huidige spraakherkenningssystemen op de Vlaamse spreektaal om hun bruikbaarheid te beoordelen. Slagen huidige speech-to-text-modellen erin een dialoog tussen twee sprekers in het Vlaams voldoende accuraat om te zetten in geschreven taal? Hoe presteren de modellen voor het Vlaams vergeleken met het Nederlands? Zijn er significante verschillen in de transcriptie van Vlaamse standaardtaal en Vlaamse regiolecten? Welke tool komt als Vlaamse spraakherkenningskampioen uit de bus voor de gegeven casus?\n\n\nOnderzoek\nOp basis van een literatuurstudie werd een keuze gemaakt uit de beschikbare data die kon gebruikt worden om de onderzoeksvragen te beantwoorden. De specifieke casus die aanleiding gaf tot het onderzoek – nl. de vraag naar een spraakherkenningsmodel dat bruikbare transcripties oplevert voor Vlaamse één-op-ééngesprekken – was doorslaggevend voor de gemaakte keuzes.\nEr werden twee datasets geselecteerd. Enerzijds gebruikte Jens de telefoondialogen en face-to-face-gesprekken uit de Corpus Gesproken Nederlands. Deze bevatten ongeveer 27 uur aan Nederlandse en Vlaamse audiodata van zowel voorbereide als spontane conversaties, ook in bepaalde regionale variaties (de zogenaamde Vlaamse tussentaal). Dit komt overeen met ongeveer 406.000 woorden. Deze gesprekken worden gekenmerkt door hun spontaniteit en indirect karakter. Anderzijds werden ook voor 4 uur aan geluidsfragmenten uit de corpus Variatie in Nederlandse Taal en Sprekers gebruikt (ongeveer 38.000 woorden), meer bepaald de set met betekenisvolle en met betekenisloze zinnen. De variatie in voorspelbaarheid van deze zinnen verhoogt hun meerwaarde voor het onderzoek. Belangrijk om op te merken is dat in beide datasets enkel één-op-ééngesprekken voorkomen en dat de geluidsfragmenten gekozen of aangepast werden zodat bepaalde verstoringen zoals ruis of slikken minder voorkwamen.\nOm te bepalen hoe goed de modellen de audio kunnen omzetten naar tekst, werd de herkende tekst vergeleken met gecontroleerde transcripties die als gold standard gelden. Per fragment werd de zogenoemde Word Error Rate (WER) berekend. Dit is een relatieve weergave van het aantal foutief herkende woorden, waarbij bijgehouden wordt hoeveel woorden worden toegevoegd, weggelaten of aangepast in een automatische transcriptie ten opzichte van de controletranscriptie. De geteste geluidsfragmenten lieten zich opsplitsen in verschillende categorieën: Algemeen Nederlands, Vlaamse standaardtaal en Vlaamse tussentaal. Die laatste categorie werd verder opgesplitst volgens regio: West-Vlaanderen, Oost-Vlaanderen, Antwerpen en Vlaams-Brabant, en Limburg. Per categorie werd vervolgens de gemiddelde WER berekend. Algemeen wordt aangenomen dat een WER van 5% tot 10% goed en van 10% tot 20% acceptabel is. Een nadeel van de gevolgde werkwijze is dat deze maat nogal streng wordt toegepast. Zo wordt bijvoorbeeld “er op” niet als correcte weergave van het tussentalige “d’rop” beschouwd. Ook worden modellen die getallen in cijfers weergeven benadeeld, aangezien de correcte tekst de getallen als voluit geschreven woorden bevat.\nNaast de keuze voor de testdata, werd ook een selectie gemaakt van de meest veelbelovende spraakmodellen voor de gegeven casus. Mee bepalend voor de keuze waren criteria zoals de mogelijkheid om Nederlandstalige audio te transcriberen – niet alle publiek beschikbare modellen kunnen dit – en het bijhorende prijskaartje. In de studie werden uiteindelijk SeamlessM4T-v2 (van Meta), Whisper Large-v3 (OpenAI), Nova-2 (Deepgram), Google STT (Google) en Chirp (Google) met elkaar vergeleken. Dit zijn allemaal zogenaamde multilinguale (meertalige) modellen. Daarom is het belangrijk om te weten op basis van welke (Nederlandstalige) data deze modellen getraind werden. Volgens de beschikbare documentatie werd Seamless getraind op 6363 uren en Whisper 2077 uren aan Nederlandse tekst (geen Vlaams). Voor de andere modellen is geen precieze informatie over specifieke Nederlandstalige training van de modellen bekend.\n\n\nResultaten\n\nBovenstaande grafieken tonen dat de onderzochte spraakmodellen niet goed presteren op Nederlandstalige audiofragmenten, met een gemiddelde WER tot 77%! Voorts valt op dat de WER voor Vlaamse fragmenten consistent hoger ligt dan voor Nederlandse (uit Nederland), met verschillen tussen de 5 en 12 procentpunten. Dat betekent dus dat de modellen slechter presteren op Vlaamse audio. Als we verder inzoomen op regionale verschillen in Vlaanderen, stellen we vast dat de geteste spraakherkenningsmodellen het meeste moeite hebben met West-Vlaamse fragmenten. Het model dat het beste presteert voor Vlaamse spraakherkenning is Chirp van Google.\nWat betekenen deze resultaten nu concreet voor de bruikbaarheid van huidige state-of-the-art spraakherkenningsmodellen voor het Vlaams? In ieder geval maakt dit onderzoek duidelijk dat de technologie nog geen echt bruikbare herkenning van Vlaamse (tussen)taal oplevert. Betekent dit dat we gewoon moeten wachten tot er betere modellen op de markt komen? Niet noodzakelijk. Zo is het – zonder te technisch te willen worden – mogelijk om bestaande taalmodellen te verbeteren voor specifieke doeleinden zonder ze volledig opnieuw te moeten trainen. Dit proces heet finetunen en is bijvoorbeeld mogelijk wanneer de broncode van de modellen die we willen verbeteren beschikbaar is. Van de geteste modellen in het onderzoek zijn Seamless van Meta en Whisper van OpenAI open source en dus modellen die in aanmerking komen om te finetunen. Een eerste poging om Whisper te finetunen specifiek voor de Vlaamse taal leverde alvast een substantiële verbetering op: de WER verminderde van 76% tot 52%. Dit is niet helemaal verrassend aangezien er voor Whisper in de trainingsfase geen gebruik gemaakt werd van Vlaamse trainingsdata. Toch valt op dat zelfs deze verbetering niet genoeg is om Chirp van Google als Nederlandse spraakherkenningskampioen van de troon te stoten."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html",
    "href": "posts/RAA-QuestionPro/index.html",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Bij het uitvoeren van online bevragingen moeten onderzoekers steeds een evenwicht vinden tussen praktisch gemak en het respecteren van de anonimiteit van respondenten. Vaak willen we gebruikmaken van verzendlijsten om uitnodigingen en herinneringen te versturen. Tegelijk moeten we vermijden dat antwoorden ooit gekoppeld kunnen worden aan de identiteit van deelnemers.\nQuestionPro biedt met Respondent Anonymity Assurance (RAA) een oplossing die precies dit mogelijk maakt.\nMeer informatie en de officiële handleiding vind je hier:\n👉 https://www.questionpro.com/help/respondent-anonymity-assurance-raa.html\n\n\n\nRAA is een functionaliteit die ervoor zorgt dat contactgegevens en survey-antwoorden volledig gescheiden verwerkt worden. Het systeem laat toe dat onderzoekers:\n\nUitnodigingen en herinneringen via e-mail sturen.\n\nVerzendlijsten efficiënt beheren, zonder dat antwoorden zichtbaar zijn op individueel niveau.\n\nData analyseren in een volledig anonieme dataset.\n\nDit betekent dat onderzoekers het beste van twee werelden krijgen: gebruiksgemak in de communicatie met deelnemers én naleving van ethische en juridische vereisten.\n\n\n\n\nVoor HOGENT-onderzoekers brengt dit verschillende voordelen:\n\nAnonimiteit gegarandeerd: deelnemers kunnen erop vertrouwen dat hun antwoorden niet herleidbaar zijn tot hun identiteit.\n\nEfficiënte opvolging: uitnodigingen en herinneringen verlopen automatisch, zonder extra administratie.\n\nCompliance ingebouwd: de aanpak ondersteunt principes zoals privacy by design, dataminimalisatie en transparantie.\n\nEenvoudige verantwoording: dankzij een heldere standaardformulering kunnen onderzoekers eenvoudig uitleggen hoe de anonimiteit verzekerd wordt.\n\n\n\n\n\nOnderstaande tekst kan rechtstreeks gebruikt worden in onderzoeksdocumentatie, zoals een aanvraag bij een ethisch comité, een datamanagementplan of een GDPR-plan:\n\nGebruik van Respondent Anonymity Assurance (RAA) via QuestionPro\nVoor deze bevraging wordt gebruikgemaakt van de Respondent Anonymity Assurance (RAA)-functionaliteit van QuestionPro (zie handleiding).\nRespondenten ontvangen uitnodigingen en eventuele herinneringen per e-mail. Contactgegevens en antwoorden worden in het systeem strikt gescheiden verwerkt. Hierdoor is er geen mogelijkheid om antwoorden te koppelen aan de identiteit van respondenten.\nDeze aanpak garandeert anonimiteit en voldoet aan de vereisten inzake vertrouwelijkheid en bescherming van persoonsgegevens.\n\n\n\n\n\nDe RAA-functionaliteit in QuestionPro maakt het eenvoudig om vragenlijsten op te zetten die zowel praktisch bruikbaar zijn voor onderzoekers als veilig en vertrouwelijk voor respondenten.\nCADS raadt onderzoekers aan om bij elke survey waar anonimiteit essentieel is, gebruik te maken van RAA. Het zorgt ervoor dat je:\n\nEenvoudig deelnemers bereikt via verzendlijsten en reminders.\n\nAnonimiteit waarborgt door een strikte scheiding tussen contactgegevens en antwoorden.\n\nEen duidelijke verantwoording hebt voor ethische commissies, datamanagementplannen en GDPR-documentatie.\n\nDoor RAA consequent te gebruiken, versterken we samen het vertrouwen van deelnemers en de kwaliteit van onderzoek aan HOGENT."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#wat-is-raa",
    "href": "posts/RAA-QuestionPro/index.html#wat-is-raa",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "RAA is een functionaliteit die ervoor zorgt dat contactgegevens en survey-antwoorden volledig gescheiden verwerkt worden. Het systeem laat toe dat onderzoekers:\n\nUitnodigingen en herinneringen via e-mail sturen.\n\nVerzendlijsten efficiënt beheren, zonder dat antwoorden zichtbaar zijn op individueel niveau.\n\nData analyseren in een volledig anonieme dataset.\n\nDit betekent dat onderzoekers het beste van twee werelden krijgen: gebruiksgemak in de communicatie met deelnemers én naleving van ethische en juridische vereisten."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#waarom-is-dit-belangrijk-voor-onderzoekers",
    "href": "posts/RAA-QuestionPro/index.html#waarom-is-dit-belangrijk-voor-onderzoekers",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Voor HOGENT-onderzoekers brengt dit verschillende voordelen:\n\nAnonimiteit gegarandeerd: deelnemers kunnen erop vertrouwen dat hun antwoorden niet herleidbaar zijn tot hun identiteit.\n\nEfficiënte opvolging: uitnodigingen en herinneringen verlopen automatisch, zonder extra administratie.\n\nCompliance ingebouwd: de aanpak ondersteunt principes zoals privacy by design, dataminimalisatie en transparantie.\n\nEenvoudige verantwoording: dankzij een heldere standaardformulering kunnen onderzoekers eenvoudig uitleggen hoe de anonimiteit verzekerd wordt."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#voorbeeldformulering-voor-onderzoeksdocumentatie",
    "href": "posts/RAA-QuestionPro/index.html#voorbeeldformulering-voor-onderzoeksdocumentatie",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "Onderstaande tekst kan rechtstreeks gebruikt worden in onderzoeksdocumentatie, zoals een aanvraag bij een ethisch comité, een datamanagementplan of een GDPR-plan:\n\nGebruik van Respondent Anonymity Assurance (RAA) via QuestionPro\nVoor deze bevraging wordt gebruikgemaakt van de Respondent Anonymity Assurance (RAA)-functionaliteit van QuestionPro (zie handleiding).\nRespondenten ontvangen uitnodigingen en eventuele herinneringen per e-mail. Contactgegevens en antwoorden worden in het systeem strikt gescheiden verwerkt. Hierdoor is er geen mogelijkheid om antwoorden te koppelen aan de identiteit van respondenten.\nDeze aanpak garandeert anonimiteit en voldoet aan de vereisten inzake vertrouwelijkheid en bescherming van persoonsgegevens."
  },
  {
    "objectID": "posts/RAA-QuestionPro/index.html#conclusie",
    "href": "posts/RAA-QuestionPro/index.html#conclusie",
    "title": "Anonimiteit verzekerd met Respondent Anonymity Assurance (RAA) in QuestionPro",
    "section": "",
    "text": "De RAA-functionaliteit in QuestionPro maakt het eenvoudig om vragenlijsten op te zetten die zowel praktisch bruikbaar zijn voor onderzoekers als veilig en vertrouwelijk voor respondenten.\nCADS raadt onderzoekers aan om bij elke survey waar anonimiteit essentieel is, gebruik te maken van RAA. Het zorgt ervoor dat je:\n\nEenvoudig deelnemers bereikt via verzendlijsten en reminders.\n\nAnonimiteit waarborgt door een strikte scheiding tussen contactgegevens en antwoorden.\n\nEen duidelijke verantwoording hebt voor ethische commissies, datamanagementplannen en GDPR-documentatie.\n\nDoor RAA consequent te gebruiken, versterken we samen het vertrouwen van deelnemers en de kwaliteit van onderzoek aan HOGENT."
  }
]